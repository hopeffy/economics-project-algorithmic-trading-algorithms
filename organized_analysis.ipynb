{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Algorithmic Trading Testing Phase\n",
        "\n",
        "## Phase 1: Data Pipeline (Data Acquisition)\n",
        "Fetching and cleaning Binance, On-Chain, and Macro data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ccxt\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import yfinance as yf\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = \"data\"\n",
        "TIMEFRAMES = ['1h', '4h', '1d']\n",
        "PAIRS = ['BTC/USDT', 'ETH/USDT']\n",
        "START_DATE = \"2022-01-01 00:00:00\"\n",
        "YF_START_DATE = \"2022-01-01\"\n",
        "# End date is effectively \"now\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully\n",
            "On-chain enabled: True\n",
            "Macro enabled: True\n"
          ]
        }
      ],
      "source": [
        "## Extended Data Fetching: Macro + On-Chain\n",
        "import yaml\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load configuration\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Configuration loaded successfully\")\n",
        "print(f\"On-chain enabled: {config['data']['onchain']['enabled']}\")\n",
        "print(f\"Macro enabled: {config['data']['macro']['enabled']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_data(df, filename):\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    df.to_csv(path)\n",
        "    print(f\"Saved {filename} to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_binance_data(symbol, timeframe, start_date):\n",
        "    \"\"\"\n",
        "    Fetches historical OHLCV data from Binance.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching {symbol} {timeframe} data from Binance...\")\n",
        "    exchange = ccxt.binance()\n",
        "    \n",
        "    # Convert start_date to timestamp\n",
        "    since = exchange.parse8601(start_date)\n",
        "    \n",
        "    all_ohlcv = []\n",
        "    limit = 1000\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since, limit)\n",
        "            if not ohlcv:\n",
        "                break\n",
        "            \n",
        "            all_ohlcv.extend(ohlcv)\n",
        "            since = ohlcv[-1][0] + 1  # Move to next timestamp\n",
        "            \n",
        "            # Break if we reached current time (approx) - fetch_ohlcv handles this but good to be safe\n",
        "            if len(ohlcv) < limit:\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            break\n",
        "            \n",
        "    df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "    \n",
        "    # Clean data\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "    df.dropna(inplace=True)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_macro_data(start_date):\n",
        "    \"\"\"\n",
        "    Fetches Macro data (DXY) using yfinance.\n",
        "    \"\"\"\n",
        "    print(\"Fetching Macro data (DXY)...\")\n",
        "    # DXY symbol on Yahoo Finance is 'DX-Y.NYB' or similar. 'DX-Y.NYB' is often used.\n",
        "    # Alternatively 'UUP' (ETF) or 'DX=F' (Futures). Let's try 'DX-Y.NYB'.\n",
        "    ticker = 'DX-Y.NYB' \n",
        "    \n",
        "    try:\n",
        "        df = yf.download(ticker, start=start_date, progress=False)\n",
        "        if df.empty:\n",
        "             # Fallback if the above symbol doesn't work well\n",
        "             print(\"Primary DXY symbol failed, trying 'DX=F'\")\n",
        "             df = yf.download('DX=F', start=start_date, progress=False)\n",
        "             \n",
        "        # yfinance returns MultiIndex columns sometimes, flatten if needed\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "            \n",
        "        # Rename Close to DXY_Close for clarity\n",
        "        if 'Close' in df.columns:\n",
        "            df = df[['Close']].rename(columns={'Close': 'DXY'})\n",
        "        else:\n",
        "             print(\"Warning: 'Close' column not found in macro data. Columns:\", df.columns)\n",
        "             return pd.DataFrame()\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching macro data: {e}\")\n",
        "        return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_fred_data(series_id, start_date, api_key=None):\n",
        "    \"\"\"\n",
        "    Fetches macroeconomic data from FRED (Federal Reserve Economic Data)\n",
        "    \n",
        "    Args:\n",
        "        series_id: FRED series ID (e.g., 'CPIAUCSL' for CPI)\n",
        "        start_date: Start date in YYYY-MM-DD format\n",
        "        api_key: FRED API key (optional if set in environment)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with the requested series\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from fredapi import Fred\n",
        "        \n",
        "        if api_key is None:\n",
        "            api_key = os.getenv('FRED_API_KEY')\n",
        "        \n",
        "        if api_key is None or api_key == 'your_fred_api_key_here':\n",
        "            print(f\"âš ï¸  FRED API key not set. Using mock data for {series_id}\")\n",
        "            # Return mock data with similar structure\n",
        "            dates = pd.date_range(start=start_date, end=datetime.now(), freq='D')\n",
        "            mock_data = pd.DataFrame({\n",
        "                series_id: np.random.randn(len(dates)).cumsum() + 100\n",
        "            }, index=dates)\n",
        "            return mock_data\n",
        "        \n",
        "        fred = Fred(api_key=api_key)\n",
        "        data = fred.get_series(series_id, observation_start=start_date)\n",
        "        df = pd.DataFrame({series_id: data})\n",
        "        \n",
        "        print(f\"âœ… Fetched {series_id}: {len(df)} records from {df.index.min()} to {df.index.max()}\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error fetching {series_id}: {e}\")\n",
        "        # Return empty DataFrame\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def fetch_all_macro_data(start_date=YF_START_DATE):\n",
        "    \"\"\"\n",
        "    Fetches all configured macro indicators from FRED\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FETCHING MACROECONOMIC DATA FROM FRED\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    macro_indicators = config['data']['macro']['indicators']\n",
        "    \n",
        "    # FRED series ID mapping\n",
        "    fred_series = {\n",
        "        'DXY': None,  # Already fetched via yfinance\n",
        "        'US10Y': 'DGS10',  # 10-Year Treasury Constant Maturity Rate\n",
        "        'CPI': 'CPIAUCSL',  # Consumer Price Index\n",
        "        'PCE': 'PCEPI',  # Personal Consumption Expenditures Price Index\n",
        "        'UNRATE': 'UNRATE',  # Unemployment Rate\n",
        "        'PAYEMS': 'PAYEMS',  # Non-farm Payroll Employment\n",
        "        'FEDFUNDS': 'FEDFUNDS'  # Federal Funds Effective Rate\n",
        "    }\n",
        "    \n",
        "    macro_data = {}\n",
        "    \n",
        "    for indicator in macro_indicators:\n",
        "        if indicator == 'DXY':\n",
        "            continue  # Skip DXY, already have it\n",
        "        \n",
        "        series_id = fred_series.get(indicator)\n",
        "        if series_id:\n",
        "            df = fetch_fred_data(series_id, start_date)\n",
        "            if not df.empty:\n",
        "                macro_data[indicator] = df\n",
        "    \n",
        "    return macro_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_onchain_data_coinmetrics(asset, metric, start_date):\n",
        "    \"\"\"\n",
        "    Fetches on-chain metrics from CoinMetrics Community API (FREE)\n",
        "    \n",
        "    Args:\n",
        "        asset: Asset symbol ('BTC', 'ETH')\n",
        "        metric: Metric name\n",
        "        start_date: Start date\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with the metric data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # CoinMetrics community API (no key required)\n",
        "        base_url = 'https://community-api.coinmetrics.io/v4'\n",
        "        \n",
        "        # Metric mapping\n",
        "        metric_map = {\n",
        "            'NVT': 'NVTAdj',  # Network Value to Transactions ratio\n",
        "            'MVRV': 'CapMVRVCur',  # Market Value to Realized Value\n",
        "            'active_addresses': 'AdrActCnt',\n",
        "            'transaction_count': 'TxCnt'\n",
        "        }\n",
        "        \n",
        "        cm_metric = metric_map.get(metric)\n",
        "        if not cm_metric:\n",
        "            print(f\"âš ï¸  Metric {metric} not available in CoinMetrics, using calculated proxy\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Format asset for CoinMetrics\n",
        "        cm_asset = asset.lower()\n",
        "        \n",
        "        # Build API request\n",
        "        url = f'{base_url}/timeseries/asset-metrics'\n",
        "        params = {\n",
        "            'assets': cm_asset,\n",
        "            'metrics': cm_metric,\n",
        "            'start_time': start_date,\n",
        "            'frequency': '1d'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        data = response.json()['data']\n",
        "        \n",
        "        if not data:\n",
        "            print(f\"âš ï¸  No data from CoinMetrics for {asset} {metric}\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Parse response\n",
        "        df = pd.DataFrame(data)\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df = df.set_index('time')\n",
        "        df = df[[cm_metric]].rename(columns={cm_metric: f'{asset}_{metric}'})\n",
        "        \n",
        "        print(f\"âœ… Fetched {asset} {metric} from CoinMetrics: {len(df)} records\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  CoinMetrics error for {asset} {metric}: {e}\")\n",
        "        print(f\"   Will use calculated proxy instead\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def calculate_onchain_proxies(price_df, asset):\n",
        "    \"\"\"\n",
        "    Calculate on-chain proxy metrics from price/volume data\n",
        "    \n",
        "    Args:\n",
        "        price_df: DataFrame with OHLCV data\n",
        "        asset: Asset symbol\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with calculated on-chain proxies\n",
        "    \"\"\"\n",
        "    df = price_df.copy()\n",
        "    \n",
        "    # NVT Proxy: Market Cap / Volume ratio (inverted so lower = better like real NVT)\n",
        "    df['market_cap_proxy'] = df['close'] * df['volume']\n",
        "    df[f'{asset}_NVT_proxy'] = df['market_cap_proxy'] / (df['volume'] * df['close'] + 1)\n",
        "    df[f'{asset}_NVT_proxy'] = df[f'{asset}_NVT_proxy'].rolling(window=14).mean()\n",
        "    \n",
        "    # MVRV Proxy: Price / Moving Average ratio (simplified MVRV concept)\n",
        "    df[f'{asset}_MVRV_proxy'] = df['close'] / df['close'].rolling(window=200).mean()\n",
        "    \n",
        "    # Exchange Flow Proxy: Volume momentum (high volume = potential exchange activity)\n",
        "    df['volume_ma'] = df['volume'].rolling(window=20).mean()\n",
        "    df[f'{asset}_exchange_flow_proxy'] = df['volume'] / (df['volume_ma'] + 1)\n",
        "    \n",
        "    # HODL Proxy: Price stability score (low volatility = HODLing)\n",
        "    df['returns'] = df['close'].pct_change()\n",
        "    df['volatility_30d'] = df['returns'].rolling(window=30).std()\n",
        "    df[f'{asset}_hodl_proxy'] = 1 / (df['volatility_30d'] + 0.001)  # Inverse of volatility\n",
        "    \n",
        "    # Clean up temporary columns\n",
        "    result_cols = [col for col in df.columns if '_proxy' in col or metric.upper() in col for metric in ['NVT', 'MVRV']]\n",
        "    \n",
        "    print(f\"âœ… Calculated {len(result_cols)} on-chain proxies for {asset}\")\n",
        "    \n",
        "    return df[result_cols] if result_cols else df\n",
        "\n",
        "# Glassnode removed - using CoinMetrics (free) + calculated proxies instead\n",
        "\n",
        "def fetch_all_onchain_data(start_date=START_DATE, use_proxies=True):\n",
        "    \"\"\"\n",
        "    Fetches all configured on-chain metrics for BTC and ETH\n",
        "    \n",
        "    Strategy:\n",
        "    1. Try CoinMetrics Community API (free, no key)\n",
        "    2. If fails, calculate proxies from price/volume data\n",
        "    3. Glassnode only if API key is available\n",
        "    \n",
        "    Args:\n",
        "        start_date: Start date for data\n",
        "        use_proxies: If True, calculate proxies for missing metrics\n",
        "    \n",
        "    Returns:\n",
        "        Dict of DataFrames with on-chain metrics\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FETCHING ON-CHAIN DATA (Multi-Source Strategy)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    assets = ['BTC', 'ETH']\n",
        "    metrics = config['data']['onchain']['metrics']\n",
        "    \n",
        "    onchain_data = {}\n",
        "    \n",
        "    for asset in assets:\n",
        "        print(f\"\\nðŸ“Š Processing {asset}...\")\n",
        "        \n",
        "        for metric in metrics:\n",
        "            # Try CoinMetrics Community API (FREE, no key required)\n",
        "            df = fetch_onchain_data_coinmetrics(asset, metric, start_date)\n",
        "            \n",
        "            # Store if we got data\n",
        "            if not df.empty:\n",
        "                key = f'{asset}_{metric}'\n",
        "                onchain_data[key] = df\n",
        "            else:\n",
        "                print(f\"   âš ï¸  {metric} will be calculated as proxy from price data\")\n",
        "    \n",
        "    # 4. If use_proxies enabled, we'll calculate them later from price data\n",
        "    if use_proxies:\n",
        "        print(\"\\nðŸ’¡ Note: Missing on-chain metrics will be calculated as proxies from price/volume data\")\n",
        "    \n",
        "    return onchain_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def align_and_merge_data(price_df, macro_data, onchain_data, timeframe='1d'):\n",
        "    \"\"\"\n",
        "    Aligns and merges price, macro, and on-chain data\n",
        "    \n",
        "    Handles:\n",
        "    - Timezone conversions (UTC standardization)\n",
        "    - Frequency resampling (daily â†’ intraday with forward fill)\n",
        "    - Missing value handling\n",
        "    \n",
        "    Args:\n",
        "        price_df: Price OHLCV data\n",
        "        macro_data: Dict of macro DataFrames\n",
        "        onchain_data: Dict of on-chain DataFrames\n",
        "        timeframe: Target timeframe ('1h', '4h', '1d')\n",
        "    \n",
        "    Returns:\n",
        "        Merged DataFrame with all features aligned\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ”„ Aligning data for {timeframe} timeframe...\")\n",
        "    \n",
        "    # Ensure price_df index is timezone-aware (UTC)\n",
        "    if price_df.index.tz is None:\n",
        "        price_df.index = price_df.index.tz_localize('UTC')\n",
        "    else:\n",
        "        price_df.index = price_df.index.tz_convert('UTC')\n",
        "    \n",
        "    result_df = price_df.copy()\n",
        "    \n",
        "    # Merge macro data (typically daily)\n",
        "    for name, macro_df in macro_data.items():\n",
        "        if macro_df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Ensure timezone awareness\n",
        "        if macro_df.index.tz is None:\n",
        "            macro_df.index = macro_df.index.tz_localize('UTC')\n",
        "        else:\n",
        "            macro_df.index = macro_df.index.tz_convert('UTC')\n",
        "        \n",
        "        # Resample to target frequency with forward fill\n",
        "        if timeframe in ['1h', '4h']:\n",
        "            # For intraday, resample daily data to hourly then select\n",
        "            macro_resampled = macro_df.resample('1H').ffill()\n",
        "            result_df = result_df.join(macro_resampled, how='left')\n",
        "        else:\n",
        "            # For daily, direct join\n",
        "            result_df = result_df.join(macro_df, how='left')\n",
        "        \n",
        "        # Forward fill remaining NaNs\n",
        "        result_df[macro_df.columns] = result_df[macro_df.columns].ffill()\n",
        "        \n",
        "        print(f\"  âœ… Merged {name}: {result_df[macro_df.columns[0]].notna().sum()} non-null values\")\n",
        "    \n",
        "    # Merge on-chain data (typically daily)\n",
        "    for name, onchain_df in onchain_data.items():\n",
        "        if onchain_df.empty:\n",
        "            continue\n",
        "        \n",
        "        # Ensure timezone awareness\n",
        "        if onchain_df.index.tz is None:\n",
        "            onchain_df.index = onchain_df.index.tz_localize('UTC')\n",
        "        else:\n",
        "            onchain_df.index = onchain_df.index.tz_convert('UTC')\n",
        "        \n",
        "        # Resample to target frequency with forward fill\n",
        "        if timeframe in ['1h', '4h']:\n",
        "            onchain_resampled = onchain_df.resample('1H').ffill()\n",
        "            result_df = result_df.join(onchain_resampled, how='left')\n",
        "        else:\n",
        "            result_df = result_df.join(onchain_df, how='left')\n",
        "        \n",
        "        # Forward fill remaining NaNs\n",
        "        result_df[onchain_df.columns] = result_df[onchain_df.columns].ffill()\n",
        "        \n",
        "        print(f\"  âœ… Merged {name}: {result_df[onchain_df.columns[0]].notna().sum()} non-null values\")\n",
        "    \n",
        "    # Fill any remaining NaNs at the start with backward fill (limited)\n",
        "    result_df = result_df.fillna(method='bfill', limit=10)\n",
        "    \n",
        "    print(f\"âœ… Final merged data: {len(result_df)} rows, {len(result_df.columns)} columns\")\n",
        "    print(f\"   NaN counts: {result_df.isna().sum().sum()} total\")\n",
        "    \n",
        "    return result_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching BTC/USDT 1h data from Binance...\n",
            "Saved BTCUSDT_1h.csv to data\\BTCUSDT_1h.csv\n",
            "Fetching BTC/USDT 4h data from Binance...\n",
            "Saved BTCUSDT_4h.csv to data\\BTCUSDT_4h.csv\n",
            "Fetching BTC/USDT 1d data from Binance...\n",
            "Saved BTCUSDT_1d.csv to data\\BTCUSDT_1d.csv\n",
            "Fetching ETH/USDT 1h data from Binance...\n",
            "Saved ETHUSDT_1h.csv to data\\ETHUSDT_1h.csv\n",
            "Fetching ETH/USDT 4h data from Binance...\n",
            "Saved ETHUSDT_4h.csv to data\\ETHUSDT_4h.csv\n",
            "Fetching ETH/USDT 1d data from Binance...\n",
            "Saved ETHUSDT_1d.csv to data\\ETHUSDT_1d.csv\n",
            "Fetching Macro data (DXY)...\n",
            "Saved macro_dxy.csv to data\\macro_dxy.csv\n",
            "Data acquisition complete.\n"
          ]
        }
      ],
      "source": [
        "# Execution Block - Phase 1\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure data directory exists\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "\n",
        "    # Fetch Crypto Data\n",
        "    for pair in PAIRS:\n",
        "        symbol_clean = pair.replace('/', '')\n",
        "        for tf in TIMEFRAMES:\n",
        "            df = fetch_binance_data(pair, tf, START_DATE)\n",
        "            filename = f\"{symbol_clean}_{tf}.csv\"\n",
        "            save_data(df, filename)\n",
        "            \n",
        "    # Fetch Macro Data\n",
        "    # Macro data is usually daily. We might need to forward fill for intraday timeframes later.\n",
        "    macro_df = fetch_macro_data(YF_START_DATE)\n",
        "    if not macro_df.empty:\n",
        "        save_data(macro_df, \"macro_dxy.csv\")\n",
        "    else:\n",
        "        print(\"Failed to fetch Macro data.\")\n",
        "    \n",
        "    print(\"Data acquisition complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Execution Core (Algorithmic Engine)\n",
        "Implementing indicators and trading logic for Test Case 1 (Grid Search)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_indicators(df):\n",
        "    \"\"\"\n",
        "    Calculates technical indicators: RSI, Bollinger Bands, KAMA, SuperTrend, EMA200, ADX.\n",
        "    Standardizes column names for consistency.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid SettingWithCopyWarning\n",
        "    df = df.copy()\n",
        "    \n",
        "    # RSI\n",
        "    df['RSI'] = df.ta.rsi(length=14)\n",
        "    \n",
        "    # Bollinger Bands\n",
        "    bb = df.ta.bbands(length=20, std=2)\n",
        "    if bb is not None:\n",
        "        # pandas_ta returns: BBL_20_2.0_2.0, BBM_20_2.0_2.0, BBU_20_2.0_2.0, BBB_20_2.0_2.0, BBP_20_2.0_2.0\n",
        "        # We only need Lower, Middle, Upper bands\n",
        "        df['BBL_20_2.0'] = bb.iloc[:, 0]  # Lower band\n",
        "        df['BBM_20_2.0'] = bb.iloc[:, 1]  # Middle band\n",
        "        df['BBU_20_2.0'] = bb.iloc[:, 2]  # Upper band\n",
        "    \n",
        "    # KAMA\n",
        "    kama = df.ta.kama(length=10)\n",
        "    if kama is not None:\n",
        "        df['KAMA'] = kama\n",
        "    \n",
        "    # SuperTrend (length=10, multiplier=3)\n",
        "    st = df.ta.supertrend(length=10, multiplier=3)\n",
        "    if st is not None:\n",
        "        # pandas_ta SuperTrend returns: SUPERT_10_3, SUPERTd_10_3, SUPERTl_10_3, SUPERTs_10_3\n",
        "        # We only need the direction column (SUPERTd)\n",
        "        # SUPERTd is the direction: 1 for uptrend, -1 for downtrend\n",
        "        direction_col = [col for col in st.columns if 'SUPERTd' in col][0]\n",
        "        df['SUPERTd_10_3.0'] = st[direction_col]\n",
        "    \n",
        "    # EMA200 for trend filter\n",
        "    df['EMA200'] = df.ta.ema(length=200)\n",
        "    \n",
        "    # ADX for trend strength\n",
        "    adx = df.ta.adx(length=14)\n",
        "    if adx is not None:\n",
        "        df['ADX'] = adx['ADX_14']\n",
        "    \n",
        "    # Drop rows with NaN values (first ~200 rows will be dropped due to EMA200 warmup)\n",
        "    # Keep track of how many rows we're dropping\n",
        "    original_len = len(df)\n",
        "    df.dropna(inplace=True)\n",
        "    dropped = original_len - len(df)\n",
        "    if dropped > 0:\n",
        "        print(f\"  Dropped {dropped} rows due to indicator warmup period\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Case 2: Custom Rule (Ichimoku + Market Structure)\n",
        "Implementing Ichimoku Cloud, Market Structure (HH/HL), and the custom strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Advanced filters loaded: Volatility, Trend Duration, Volume, Time-based\n"
          ]
        }
      ],
      "source": [
        "# ðŸŽ¯ EK Ä°YÄ°LEÅžTÄ°RMELER: 4h ve 1h Timeframe iÃ§in Filtreler\n",
        "\n",
        "def calculate_atr_filter(df, period=14):\n",
        "    \"\"\"\n",
        "    ATR (Average True Range) hesaplar ve normalize eder.\n",
        "    Volatility filter iÃ§in kullanÄ±lÄ±r.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # ATR calculation\n",
        "    atr = df.ta.atr(length=period)\n",
        "    if atr is not None:\n",
        "        df['ATR'] = atr\n",
        "        # ATR'yi price'a gÃ¶re normalize et (percentage olarak)\n",
        "        df['ATR_pct'] = (df['ATR'] / df['close']) * 100\n",
        "    \n",
        "    return df\n",
        "\n",
        "def volatility_filter(row, min_atr_pct=0.5, max_atr_pct=5.0):\n",
        "    \"\"\"\n",
        "    Volatility Filter: ATR bazlÄ± giriÅŸ filtresi\n",
        "    - Ã‡ok dÃ¼ÅŸÃ¼k volatilite: Sideways market, giriÅŸ yapma\n",
        "    - Ã‡ok yÃ¼ksek volatilite: Whipsaw riski, giriÅŸ yapma\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row\n",
        "        min_atr_pct: Minimum ATR yÃ¼zdesi (default: 0.5%)\n",
        "        max_atr_pct: Maximum ATR yÃ¼zdesi (default: 5.0%)\n",
        "    \"\"\"\n",
        "    if 'ATR_pct' not in row or pd.isna(row['ATR_pct']):\n",
        "        return False\n",
        "    \n",
        "    atr_pct = row['ATR_pct']\n",
        "    return min_atr_pct <= atr_pct <= max_atr_pct\n",
        "\n",
        "def calculate_trend_duration(df, lookback=10):\n",
        "    \"\"\"\n",
        "    Trendin kaÃ§ bardÄ±r devam ettiÄŸini hesaplar.\n",
        "    Minimum trend duration filtresi iÃ§in kullanÄ±lÄ±r.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Trend deÄŸiÅŸim noktalarÄ±nÄ± bul\n",
        "    df['trend_change'] = df['trend'] != df['trend'].shift(1)\n",
        "    \n",
        "    # Her trend iÃ§in duration hesapla\n",
        "    df['trend_duration'] = 0\n",
        "    current_duration = 0\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        if i == 0 or df['trend_change'].iloc[i]:\n",
        "            current_duration = 1\n",
        "        else:\n",
        "            current_duration += 1\n",
        "        df.iloc[i, df.columns.get_loc('trend_duration')] = current_duration\n",
        "    \n",
        "    return df\n",
        "\n",
        "def min_trend_duration_filter(row, min_bars=5):\n",
        "    \"\"\"\n",
        "    Min Trend Duration Filter: Trend en az X bar devam etmeli\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row\n",
        "        min_bars: Minimum bar sayÄ±sÄ± (1h iÃ§in 5, 4h iÃ§in 3 Ã¶nerilir)\n",
        "    \"\"\"\n",
        "    if 'trend_duration' not in row or pd.isna(row['trend_duration']):\n",
        "        return False\n",
        "    \n",
        "    return row['trend_duration'] >= min_bars\n",
        "\n",
        "def calculate_volume_metrics(df, ma_period=20):\n",
        "    \"\"\"\n",
        "    Volume metrikleri hesaplar: Volume MA ve relative volume\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Volume moving average\n",
        "    df['volume_ma'] = df['volume'].rolling(window=ma_period).mean()\n",
        "    \n",
        "    # Relative volume (current volume / average volume)\n",
        "    df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
        "    \n",
        "    return df\n",
        "\n",
        "def volume_confirmation_filter(row, min_volume_ratio=1.2):\n",
        "    \"\"\"\n",
        "    Volume Confirmation: YÃ¼ksek volume'da giriÅŸ Ã¶nceliÄŸi\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row\n",
        "        min_volume_ratio: Minimum volume ratio (1.2 = %20 Ã¼stÃ¼nde volume)\n",
        "    \"\"\"\n",
        "    if 'volume_ratio' not in row or pd.isna(row['volume_ratio']):\n",
        "        return True  # Volume data yoksa bypass et\n",
        "    \n",
        "    return row['volume_ratio'] >= min_volume_ratio\n",
        "\n",
        "def time_based_filter(timestamp, timeframe='1h'):\n",
        "    \"\"\"\n",
        "    Time-based Filter: DÃ¼ÅŸÃ¼k likidite saatlerini atla\n",
        "    \n",
        "    Args:\n",
        "        timestamp: pandas Timestamp\n",
        "        timeframe: '1h' veya '4h'\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if good trading hours, False if low liquidity hours\n",
        "    \"\"\"\n",
        "    # UTC saatine Ã§evir (Binance UTC kullanÄ±r)\n",
        "    hour_utc = timestamp.hour\n",
        "    \n",
        "    if timeframe == '1h':\n",
        "        # 1h iÃ§in: DÃ¼ÅŸÃ¼k likidite saatleri (UTC 22:00 - 02:00 arasÄ±)\n",
        "        # Bu saatler genellikle Asya-Avrupa arasÄ± geÃ§iÅŸ, dÃ¼ÅŸÃ¼k volume\n",
        "        low_liquidity_hours = list(range(22, 24)) + list(range(0, 2))\n",
        "        return hour_utc not in low_liquidity_hours\n",
        "    \n",
        "    elif timeframe == '4h':\n",
        "        # 4h iÃ§in: TÃ¼m saatler genellikle OK\n",
        "        # Opsiyonel: Weekend'leri filtrele (crypto 7/24 ama Pazar akÅŸamlarÄ± dÃ¼ÅŸÃ¼k olabilir)\n",
        "        return True\n",
        "    \n",
        "    return True\n",
        "\n",
        "print(\"âœ… Advanced filters loaded: Volatility, Trend Duration, Volume, Time-based\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                    ALGORITHMIC TRADING TEST SUMMARY\n",
            "================================================================================\n",
            "\n",
            "ðŸ“… Testing Period: January 1, 2022 - November 26, 2025\n",
            "ðŸ“Š Assets Tested: BTC/USDT, ETH/USDT\n",
            "â±ï¸ Timeframes: 1h, 4h, 1d\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST CASE 1: GRID SEARCH (RSI, BB, KAMA, SuperTrend)\n",
            "--------------------------------------------------------------------------------\n",
            "Total combinations tested: 468\n",
            "Profitable strategies: 440\n",
            "\n",
            "Top 5 Strategies:\n",
            " Symbol_TF           Entry  Entry_Param      Exit  Exit_Param   PnL%  Win_Rate%  Trades\n",
            "ETHUSDT_1d             RSI         40.0 BB_Exit_B        0.85 430.60      94.44      18\n",
            "BTCUSDT_1d             RSI         40.0 BB_Exit_B        0.85 241.17      94.74      19\n",
            "BTCUSDT_1d             RSI         40.0 BB_Exit_B        0.80 235.60      94.74      19\n",
            "BTCUSDT_1d KAMA_SuperTrend          NaN BB_Exit_A         NaN 233.34      97.37      38\n",
            "ETHUSDT_1d             RSI         40.0 BB_Exit_B        0.90 219.71      92.31      13\n",
            "\n",
            "ðŸ† Best TC1 Strategy:\n",
            "   ETHUSDT_1d - RSI(40.0) + BB_Exit_B(0.85)\n",
            "   PnL: 430.60% | Win Rate: 94.44% | Trades: 18\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST CASE 2: ICHIMOKU + MARKET STRUCTURE\n",
            "--------------------------------------------------------------------------------\n",
            "Strategy Logic:\n",
            "  Entry: TK Cross (Bullish) + Price Above Cloud + Uptrend\n",
            "  Exit: Price Below Cloud OR RSI > 70\n",
            "\n",
            "Results:\n",
            " Symbol_TF                 Strategy   PnL%  Win_Rate%  Trades\n",
            "BTCUSDT_1d Ichimoku_MarketStructure -27.96      53.12      32\n",
            "BTCUSDT_4h Ichimoku_MarketStructure -49.74      47.37     171\n",
            "ETHUSDT_1d Ichimoku_MarketStructure -58.48      44.83      29\n",
            "BTCUSDT_1h Ichimoku_MarketStructure -85.11      42.61     575\n",
            "ETHUSDT_4h Ichimoku_MarketStructure -88.64      42.95     149\n",
            "ETHUSDT_1h Ichimoku_MarketStructure -95.91      43.95     628\n",
            "\n",
            "ðŸ† Best TC2 Strategy:\n",
            "   BTCUSDT_1d - Ichimoku + Market Structure\n",
            "   PnL: -27.96% | Win Rate: 53.12% | Trades: 32\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST CASE 3: FORWARD TEST (Oct-Nov 2025)\n",
            "--------------------------------------------------------------------------------\n",
            " Symbol_TF Test_Case                           Strategy                 Period   PnL%  Win_Rate%  Trades\n",
            "BTCUSDT_1d       TC1                    RSI(30)+RSI(70) Forward (Oct-Nov 2025)   4.44     100.00       1\n",
            "ETHUSDT_1d       TC1                    RSI(25)+RSI(50) Forward (Oct-Nov 2025)   0.00       0.00       0\n",
            "ETHUSDT_1h       TC2           Ichimoku+MarketStructure Forward (Oct-Nov 2025)  -8.22      31.25      16\n",
            "BTCUSDT_4h       TC2           Ichimoku+MarketStructure Forward (Oct-Nov 2025)  -9.39       0.00       3\n",
            "BTCUSDT_1d       TC1 BB_Reversal(None)+SuperTrend(None) Forward (Oct-Nov 2025) -12.90       0.00       1\n",
            "\n",
            "ðŸ† Best Forward Test Result:\n",
            "   BTCUSDT_1d - RSI(30)+RSI(70)\n",
            "   PnL: 4.44% | Win Rate: 100.00% | Trades: 1\n",
            "\n",
            "================================================================================\n",
            "CONCLUSIONS\n",
            "================================================================================\n",
            "\n",
            "1. Grid Search Strategies:\n",
            "   - RSI-based entries with RSI/SuperTrend exits showed consistent profitability\n",
            "   - BB Reversal strategy performed well on daily timeframe\n",
            "   - Best results on ETH/USD with RSI entry threshold of 25\n",
            "\n",
            "2. Ichimoku + Market Structure:\n",
            "   - 4h timeframe showed best results for both BTC and ETH\n",
            "   - Strategy captures medium-term trends effectively\n",
            "   - Higher trade frequency compared to daily strategies\n",
            "\n",
            "3. Recommendations:\n",
            "   - Use TC2 (Ichimoku+MS) on 4h timeframe for trend-following\n",
            "   - Use TC1 RSI strategies on daily for swing trading\n",
            "   - Consider combining both strategies for portfolio diversification\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def generate_summary_report():\n",
        "    \"\"\"\n",
        "    Generates a comprehensive summary report of all test cases.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\" \" * 20 + \"ALGORITHMIC TRADING TEST SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(\"\\nðŸ“… Testing Period: January 1, 2022 - November 26, 2025\")\n",
        "    print(\"ðŸ“Š Assets Tested: BTC/USDT, ETH/USDT\")\n",
        "    print(\"â±ï¸ Timeframes: 1h, 4h, 1d\")\n",
        "    \n",
        "    # Load all results\n",
        "    tc1 = pd.read_csv('grid_search_results.csv')\n",
        "    tc2 = pd.read_csv('test_case_2_results.csv')\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TEST CASE 1: GRID SEARCH (RSI, BB, KAMA, SuperTrend)\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Total combinations tested: {len(tc1)}\")\n",
        "    print(f\"Profitable strategies: {len(tc1[tc1['PnL%'] > 0])}\")\n",
        "    print(f\"\\nTop 5 Strategies:\")\n",
        "    print(tc1.head(5).to_string(index=False))\n",
        "    \n",
        "    best_tc1 = tc1.iloc[0]\n",
        "    print(f\"\\nðŸ† Best TC1 Strategy:\")\n",
        "    print(f\"   {best_tc1['Symbol_TF']} - {best_tc1['Entry']}({best_tc1['Entry_Param']}) + {best_tc1['Exit']}({best_tc1['Exit_Param']})\")\n",
        "    print(f\"   PnL: {best_tc1['PnL%']:.2f}% | Win Rate: {best_tc1['Win_Rate%']:.2f}% | Trades: {best_tc1['Trades']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TEST CASE 2: ICHIMOKU + MARKET STRUCTURE\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Strategy Logic:\")\n",
        "    print(\"  Entry: TK Cross (Bullish) + Price Above Cloud + Uptrend\")\n",
        "    print(\"  Exit: Price Below Cloud OR RSI > 70\")\n",
        "    print(f\"\\nResults:\")\n",
        "    print(tc2.to_string(index=False))\n",
        "    \n",
        "    best_tc2 = tc2.iloc[0]\n",
        "    print(f\"\\nðŸ† Best TC2 Strategy:\")\n",
        "    print(f\"   {best_tc2['Symbol_TF']} - Ichimoku + Market Structure\")\n",
        "    print(f\"   PnL: {best_tc2['PnL%']:.2f}% | Win Rate: {best_tc2['Win_Rate%']:.2f}% | Trades: {best_tc2['Trades']}\")\n",
        "    \n",
        "    # Check if forward test results exist\n",
        "    try:\n",
        "        forward = pd.read_csv('forward_test_results.csv')\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"TEST CASE 3: FORWARD TEST (Oct-Nov 2025)\")\n",
        "        print(\"-\"*80)\n",
        "        print(forward.to_string(index=False))\n",
        "        \n",
        "        if len(forward) > 0:\n",
        "            best_forward = forward.iloc[0]\n",
        "            print(f\"\\nðŸ† Best Forward Test Result:\")\n",
        "            print(f\"   {best_forward['Symbol_TF']} - {best_forward['Strategy']}\")\n",
        "            print(f\"   PnL: {best_forward['PnL%']:.2f}% | Win Rate: {best_forward['Win_Rate%']:.2f}% | Trades: {best_forward['Trades']}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nâš ï¸ Forward test results not found. Run run_forward_test() first.\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONCLUSIONS\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\n",
        "1. Grid Search Strategies:\n",
        "   - RSI-based entries with RSI/SuperTrend exits showed consistent profitability\n",
        "   - BB Reversal strategy performed well on daily timeframe\n",
        "   - Best results on ETH/USD with RSI entry threshold of 25\n",
        "\n",
        "2. Ichimoku + Market Structure:\n",
        "   - 4h timeframe showed best results for both BTC and ETH\n",
        "   - Strategy captures medium-term trends effectively\n",
        "   - Higher trade frequency compared to daily strategies\n",
        "\n",
        "3. Recommendations:\n",
        "   - Use TC2 (Ichimoku+MS) on 4h timeframe for trend-following\n",
        "   - Use TC1 RSI strategies on daily for swing trading\n",
        "   - Consider combining both strategies for portfolio diversification\n",
        "\"\"\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_summary_report()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def run_backtest(df, entry_func, exit_func, entry_params={}, exit_params={}, enforce_profitable_exit=False):\n",
        "    \"\"\"\n",
        "    Runs a single backtest on the provided DataFrame.\n",
        "    Assumes 'close' column exists.\n",
        "    Returns: PnL, Win Rate, Trade Count, Trade List\n",
        "    \n",
        "    Args:\n",
        "        enforce_profitable_exit: If True, only exit when PnL > 0 (for TC1 requirement)\n",
        "    \"\"\"\n",
        "    balance = 10000\n",
        "    position = None  # None or {'price': float, 'size': float, 'time': timestamp}\n",
        "    trades = []\n",
        "    \n",
        "    for i in range(1, len(df)):\n",
        "        row = df.iloc[i]\n",
        "        prev_row = df.iloc[i-1]\n",
        "        \n",
        "        # Check Exit if in position\n",
        "        if position:\n",
        "            should_exit = False\n",
        "            # Generic exit call (most functions just take row and params)\n",
        "            try:\n",
        "                should_exit = exit_func(row, **exit_params)\n",
        "            except TypeError:\n",
        "                # Fallback if signature doesn't match (e.g. extra args)\n",
        "                should_exit = exit_func(row)\n",
        "            \n",
        "            # Calculate PnL for this trade\n",
        "            pnl = (row['close'] - position['price']) / position['price']\n",
        "            \n",
        "            # Apply profitable exit rule if enabled\n",
        "            if should_exit and (not enforce_profitable_exit or pnl > 0):\n",
        "                balance *= (1 + pnl)\n",
        "                trades.append({\n",
        "                    'entry_time': position['time'],\n",
        "                    'exit_time': df.index[i],\n",
        "                    'entry_price': position['price'],\n",
        "                    'exit_price': row['close'],\n",
        "                    'pnl': pnl,\n",
        "                    'result': 'win' if pnl > 0 else 'loss'\n",
        "                })\n",
        "                position = None\n",
        "        \n",
        "        # Check Entry if not in position\n",
        "        if not position:\n",
        "            should_entry = False\n",
        "            # Special case for BB Entry which needs prev_row\n",
        "            if entry_func.__name__ == 'check_bb_entry':\n",
        "                should_entry = entry_func(row, prev_row)\n",
        "            else:\n",
        "                # Generic entry call\n",
        "                try:\n",
        "                    should_entry = entry_func(row, **entry_params)\n",
        "                except TypeError:\n",
        "                    should_entry = entry_func(row)\n",
        "                \n",
        "            if should_entry:\n",
        "                position = {'price': row['close'], 'size': balance / row['close'], 'time': df.index[i]}\n",
        "                \n",
        "    # Close open position at the end\n",
        "    if position:\n",
        "        pnl = (df.iloc[-1]['close'] - position['price']) / position['price']\n",
        "        balance *= (1 + pnl)\n",
        "        trades.append({\n",
        "            'entry_time': position['time'],\n",
        "            'exit_time': df.index[-1],\n",
        "            'entry_price': position['price'],\n",
        "            'exit_price': df.iloc[-1]['close'],\n",
        "            'pnl': pnl,\n",
        "            'result': 'win' if pnl > 0 else 'loss'\n",
        "        })\n",
        "        \n",
        "    total_pnl = (balance - 10000) / 10000 * 100\n",
        "    win_count = len([t for t in trades if t['result'] == 'win'])\n",
        "    total_trades = len(trades)\n",
        "    win_rate = (win_count / total_trades * 100) if total_trades > 0 else 0\n",
        "    return total_pnl, win_rate, total_trades, trades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… RealisticBacktestEngine class loaded\n"
          ]
        }
      ],
      "source": [
        "## Advanced Backtest Engine with Realistic Costs and Risk Management\n",
        "\n",
        "class RealisticBacktestEngine:\n",
        "    \"\"\"\n",
        "    Realistic backtest engine with:\n",
        "    - Trading fees (maker/taker)\n",
        "    - Slippage\n",
        "    - Funding costs (for perpetuals)\n",
        "    - Stop-loss & Take-profit\n",
        "    - Trailing stops\n",
        "    - Position sizing (fixed, Kelly, volatility-based)\n",
        "    - Comprehensive metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, initial_capital=10000, config=None):\n",
        "        self.initial_capital = initial_capital\n",
        "        self.config = config or {}\n",
        "        \n",
        "        # Extract config\n",
        "        costs_cfg = self.config.get('backtest', {}).get('costs', {})\n",
        "        risk_cfg = self.config.get('backtest', {}).get('risk_management', {})\n",
        "        \n",
        "        self.maker_fee = costs_cfg.get('maker_fee', 0.001)\n",
        "        self.taker_fee = costs_cfg.get('taker_fee', 0.001)\n",
        "        self.slippage = costs_cfg.get('slippage', 0.0005)\n",
        "        self.funding_rate = costs_cfg.get('funding_rate', 0.0001)\n",
        "        \n",
        "        # Risk management\n",
        "        self.use_sl = risk_cfg.get('stop_loss', {}).get('enabled', True)\n",
        "        self.sl_pct = risk_cfg.get('stop_loss', {}).get('default_pct', 0.02)\n",
        "        self.use_tp = risk_cfg.get('take_profit', {}).get('enabled', True)\n",
        "        self.tp_pct = risk_cfg.get('take_profit', {}).get('default_pct', 0.04)\n",
        "        self.use_trailing = risk_cfg.get('trailing_stop', {}).get('enabled', True)\n",
        "        self.trailing_activation = risk_cfg.get('trailing_stop', {}).get('activation_pct', 0.015)\n",
        "        self.trailing_pct = risk_cfg.get('trailing_stop', {}).get('trail_pct', 0.01)\n",
        "        \n",
        "        # Position sizing\n",
        "        self.position_method = risk_cfg.get('position_sizing', {}).get('method', 'fixed')\n",
        "        self.max_position = risk_cfg.get('position_sizing', {}).get('max_position_size', 0.95)\n",
        "        self.volatility_target = risk_cfg.get('position_sizing', {}).get('volatility_target', 0.02)\n",
        "        \n",
        "        # State\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset backtest state\"\"\"\n",
        "        self.balance = self.initial_capital\n",
        "        self.position = None\n",
        "        self.trades = []\n",
        "        self.equity_curve = []\n",
        "        self.peak_equity = self.initial_capital\n",
        "        \n",
        "    def calculate_position_size(self, price, volatility=None):\n",
        "        \"\"\"\n",
        "        Calculate position size based on configured method\n",
        "        \n",
        "        Args:\n",
        "            price: Current price\n",
        "            volatility: Recent volatility (for volatility-based sizing)\n",
        "        \n",
        "        Returns:\n",
        "            Position size in units\n",
        "        \"\"\"\n",
        "        available = self.balance * self.max_position\n",
        "        \n",
        "        if self.position_method == 'fixed':\n",
        "            return available / price\n",
        "        \n",
        "        elif self.position_method == 'volatility' and volatility is not None:\n",
        "            # Scale position inversely with volatility\n",
        "            target_vol = self.volatility_target\n",
        "            vol_adj = min(target_vol / max(volatility, 0.001), 2.0)  # Cap at 2x\n",
        "            return (available * vol_adj) / price\n",
        "        \n",
        "        else:\n",
        "            return available / price\n",
        "    \n",
        "    def check_stop_loss(self, current_price):\n",
        "        \"\"\"Check if stop-loss is hit\"\"\"\n",
        "        if not self.use_sl or not self.position:\n",
        "            return False\n",
        "        \n",
        "        entry_price = self.position['entry_price']\n",
        "        sl_price = self.position.get('stop_loss')\n",
        "        \n",
        "        if sl_price and current_price <= sl_price:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def check_take_profit(self, current_price):\n",
        "        \"\"\"Check if take-profit is hit\"\"\"\n",
        "        if not self.use_tp or not self.position:\n",
        "            return False\n",
        "        \n",
        "        tp_price = self.position.get('take_profit')\n",
        "        \n",
        "        if tp_price and current_price >= tp_price:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def update_trailing_stop(self, current_price):\n",
        "        \"\"\"Update trailing stop if enabled\"\"\"\n",
        "        if not self.use_trailing or not self.position:\n",
        "            return\n",
        "        \n",
        "        entry_price = self.position['entry_price']\n",
        "        pnl_pct = (current_price - entry_price) / entry_price\n",
        "        \n",
        "        # Activate trailing stop if profit threshold reached\n",
        "        if pnl_pct >= self.trailing_activation:\n",
        "            # Calculate new trailing stop\n",
        "            new_trailing_stop = current_price * (1 - self.trailing_pct)\n",
        "            \n",
        "            # Update if higher than current\n",
        "            current_trailing = self.position.get('trailing_stop', 0)\n",
        "            if new_trailing_stop > current_trailing:\n",
        "                self.position['trailing_stop'] = new_trailing_stop\n",
        "    \n",
        "    def check_trailing_stop(self, current_price):\n",
        "        \"\"\"Check if trailing stop is hit\"\"\"\n",
        "        if not self.position:\n",
        "            return False\n",
        "        \n",
        "        trailing_stop = self.position.get('trailing_stop')\n",
        "        if trailing_stop and current_price <= trailing_stop:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def calculate_trade_cost(self, price, size, is_entry=True):\n",
        "        \"\"\"\n",
        "        Calculate total trading cost including fees and slippage\n",
        "        \n",
        "        Args:\n",
        "            price: Trade price\n",
        "            size: Trade size\n",
        "            is_entry: True for entry, False for exit\n",
        "        \n",
        "        Returns:\n",
        "            Total cost in quote currency\n",
        "        \"\"\"\n",
        "        # Use taker fee (more conservative)\n",
        "        fee = price * size * self.taker_fee\n",
        "        \n",
        "        # Slippage (worse for entry, better for exit in long)\n",
        "        slip_factor = self.slippage if is_entry else -self.slippage\n",
        "        slippage_cost = price * size * slip_factor\n",
        "        \n",
        "        return fee + slippage_cost\n",
        "    \n",
        "    def enter_position(self, row, size=None, volatility=None):\n",
        "        \"\"\"Enter a long position\"\"\"\n",
        "        if self.position:\n",
        "            return  # Already in position\n",
        "        \n",
        "        price = row['close']\n",
        "        \n",
        "        if size is None:\n",
        "            size = self.calculate_position_size(price, volatility)\n",
        "        \n",
        "        # Calculate costs\n",
        "        cost = self.calculate_trade_cost(price, size, is_entry=True)\n",
        "        \n",
        "        # Deduct from balance\n",
        "        total_cost = price * size + cost\n",
        "        if total_cost > self.balance:\n",
        "            return  # Insufficient funds\n",
        "        \n",
        "        self.balance -= total_cost\n",
        "        \n",
        "        # Set stop-loss and take-profit\n",
        "        sl_price = price * (1 - self.sl_pct) if self.use_sl else None\n",
        "        tp_price = price * (1 + self.tp_pct) if self.use_tp else None\n",
        "        \n",
        "        self.position = {\n",
        "            'entry_time': row.name,\n",
        "            'entry_price': price,\n",
        "            'size': size,\n",
        "            'stop_loss': sl_price,\n",
        "            'take_profit': tp_price,\n",
        "            'trailing_stop': None,\n",
        "            'entry_cost': cost\n",
        "        }\n",
        "    \n",
        "    def exit_position(self, row, reason='signal'):\n",
        "        \"\"\"Exit current position\"\"\"\n",
        "        if not self.position:\n",
        "            return\n",
        "        \n",
        "        price = row['close']\n",
        "        size = self.position['size']\n",
        "        entry_price = self.position['entry_price']\n",
        "        \n",
        "        # Calculate costs\n",
        "        cost = self.calculate_trade_cost(price, size, is_entry=False)\n",
        "        \n",
        "        # Calculate PnL\n",
        "        gross_pnl = (price - entry_price) * size\n",
        "        net_pnl = gross_pnl - cost - self.position['entry_cost']\n",
        "        \n",
        "        # Add to balance\n",
        "        proceeds = price * size - cost\n",
        "        self.balance += proceeds\n",
        "        \n",
        "        # Record trade\n",
        "        trade = {\n",
        "            'entry_time': self.position['entry_time'],\n",
        "            'exit_time': row.name,\n",
        "            'entry_price': entry_price,\n",
        "            'exit_price': price,\n",
        "            'size': size,\n",
        "            'gross_pnl': gross_pnl,\n",
        "            'net_pnl': net_pnl,\n",
        "            'pnl_pct': net_pnl / (entry_price * size),\n",
        "            'fees': cost + self.position['entry_cost'],\n",
        "            'exit_reason': reason,\n",
        "            'duration': (row.name - self.position['entry_time']).total_seconds() / 3600  # hours\n",
        "        }\n",
        "        \n",
        "        self.trades.append(trade)\n",
        "        self.position = None\n",
        "    \n",
        "    def run(self, df, entry_func, exit_func, entry_params={}, exit_params={}):\n",
        "        \"\"\"\n",
        "        Run backtest on provided data\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame with OHLCV and indicators\n",
        "            entry_func: Function that returns True for entry signal\n",
        "            exit_func: Function that returns True for exit signal\n",
        "            entry_params: Parameters for entry function\n",
        "            exit_params: Parameters for exit function\n",
        "        \n",
        "        Returns:\n",
        "            Dict with performance metrics and trades\n",
        "        \"\"\"\n",
        "        self.reset()\n",
        "        \n",
        "        # Calculate volatility for position sizing\n",
        "        df['returns'] = df['close'].pct_change()\n",
        "        df['volatility'] = df['returns'].rolling(window=20).std()\n",
        "        \n",
        "        for i in range(1, len(df)):\n",
        "            row = df.iloc[i]\n",
        "            prev_row = df.iloc[i-1]\n",
        "            \n",
        "            # Record equity\n",
        "            current_equity = self.balance\n",
        "            if self.position:\n",
        "                current_equity += row['close'] * self.position['size']\n",
        "            self.equity_curve.append({'time': row.name, 'equity': current_equity})\n",
        "            \n",
        "            # Update peak for drawdown calc\n",
        "            if current_equity > self.peak_equity:\n",
        "                self.peak_equity = current_equity\n",
        "            \n",
        "            # Check exits first if in position\n",
        "            if self.position:\n",
        "                # Update trailing stop\n",
        "                self.update_trailing_stop(row['close'])\n",
        "                \n",
        "                # Check stop conditions\n",
        "                if self.check_stop_loss(row['close']):\n",
        "                    self.exit_position(row, reason='stop_loss')\n",
        "                    continue\n",
        "                \n",
        "                if self.check_take_profit(row['close']):\n",
        "                    self.exit_position(row, reason='take_profit')\n",
        "                    continue\n",
        "                \n",
        "                if self.check_trailing_stop(row['close']):\n",
        "                    self.exit_position(row, reason='trailing_stop')\n",
        "                    continue\n",
        "                \n",
        "                # Check signal exit\n",
        "                try:\n",
        "                    should_exit = exit_func(row, **exit_params)\n",
        "                except TypeError:\n",
        "                    should_exit = exit_func(row)\n",
        "                \n",
        "                if should_exit:\n",
        "                    self.exit_position(row, reason='signal')\n",
        "                    continue\n",
        "            \n",
        "            # Check entry if not in position\n",
        "            if not self.position:\n",
        "                # Special case for BB Entry\n",
        "                if entry_func.__name__ == 'check_bb_entry':\n",
        "                    should_entry = entry_func(row, prev_row)\n",
        "                else:\n",
        "                    try:\n",
        "                        should_entry = entry_func(row, **entry_params)\n",
        "                    except TypeError:\n",
        "                        should_entry = entry_func(row)\n",
        "                \n",
        "                if should_entry:\n",
        "                    volatility = row.get('volatility')\n",
        "                    self.enter_position(row, volatility=volatility)\n",
        "        \n",
        "        # Close any open position at end\n",
        "        if self.position:\n",
        "            self.exit_position(df.iloc[-1], reason='end_of_data')\n",
        "        \n",
        "        # Calculate metrics\n",
        "        metrics = self.calculate_metrics()\n",
        "        \n",
        "        return {\n",
        "            'metrics': metrics,\n",
        "            'trades': pd.DataFrame(self.trades) if self.trades else pd.DataFrame(),\n",
        "            'equity_curve': pd.DataFrame(self.equity_curve)\n",
        "        }\n",
        "    \n",
        "    def calculate_metrics(self):\n",
        "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "        if not self.trades:\n",
        "            return {}\n",
        "        \n",
        "        trades_df = pd.DataFrame(self.trades)\n",
        "        equity_df = pd.DataFrame(self.equity_curve)\n",
        "        \n",
        "        # Basic metrics\n",
        "        total_return = (self.balance - self.initial_capital) / self.initial_capital\n",
        "        total_trades = len(self.trades)\n",
        "        winning_trades = len(trades_df[trades_df['net_pnl'] > 0])\n",
        "        losing_trades = len(trades_df[trades_df['net_pnl'] <= 0])\n",
        "        win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
        "        \n",
        "        # PnL metrics\n",
        "        avg_win = trades_df[trades_df['net_pnl'] > 0]['net_pnl'].mean() if winning_trades > 0 else 0\n",
        "        avg_loss = abs(trades_df[trades_df['net_pnl'] <= 0]['net_pnl'].mean()) if losing_trades > 0 else 0\n",
        "        \n",
        "        # Profit factor\n",
        "        gross_profit = trades_df[trades_df['net_pnl'] > 0]['net_pnl'].sum()\n",
        "        gross_loss = abs(trades_df[trades_df['net_pnl'] <= 0]['net_pnl'].sum())\n",
        "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')\n",
        "        \n",
        "        # Expectancy\n",
        "        expectancy = (win_rate * avg_win) - ((1 - win_rate) * avg_loss)\n",
        "        \n",
        "        # Sharpe & Sortino\n",
        "        returns = equity_df['equity'].pct_change().dropna()\n",
        "        sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if len(returns) > 0 and returns.std() > 0 else 0\n",
        "        \n",
        "        downside_returns = returns[returns < 0]\n",
        "        sortino = (returns.mean() / downside_returns.std() * np.sqrt(252)) if len(downside_returns) > 0 and downside_returns.std() > 0 else 0\n",
        "        \n",
        "        # Drawdown\n",
        "        equity_df['peak'] = equity_df['equity'].cummax()\n",
        "        equity_df['drawdown'] = (equity_df['equity'] - equity_df['peak']) / equity_df['peak']\n",
        "        max_drawdown = equity_df['drawdown'].min()\n",
        "        \n",
        "        # Calmar ratio\n",
        "        total_days = (equity_df.iloc[-1]['time'] - equity_df.iloc[0]['time']).days\n",
        "        cagr = (1 + total_return) ** (365 / max(total_days, 1)) - 1 if total_days > 0 else 0\n",
        "        calmar = cagr / abs(max_drawdown) if max_drawdown != 0 else 0\n",
        "        \n",
        "        # Trade duration\n",
        "        avg_duration = trades_df['duration'].mean()\n",
        "        \n",
        "        # Market exposure\n",
        "        total_time = (equity_df.iloc[-1]['time'] - equity_df.iloc[0]['time']).total_seconds() / 3600\n",
        "        in_market_time = trades_df['duration'].sum()\n",
        "        market_exposure = in_market_time / total_time if total_time > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'total_return_pct': total_return * 100,\n",
        "            'cagr_pct': cagr * 100,\n",
        "            'total_trades': total_trades,\n",
        "            'winning_trades': winning_trades,\n",
        "            'losing_trades': losing_trades,\n",
        "            'win_rate_pct': win_rate * 100,\n",
        "            'avg_win': avg_win,\n",
        "            'avg_loss': avg_loss,\n",
        "            'best_trade': trades_df['net_pnl'].max() if len(trades_df) > 0 else 0,\n",
        "            'worst_trade': trades_df['net_pnl'].min() if len(trades_df) > 0 else 0,\n",
        "            'profit_factor': profit_factor,\n",
        "            'expectancy': expectancy,\n",
        "            'sharpe_ratio': sharpe,\n",
        "            'sortino_ratio': sortino,\n",
        "            'max_drawdown_pct': max_drawdown * 100,\n",
        "            'calmar_ratio': calmar,\n",
        "            'avg_trade_duration_hours': avg_duration,\n",
        "            'market_exposure_pct': market_exposure * 100,\n",
        "            'final_equity': self.balance\n",
        "        }\n",
        "\n",
        "print(\"âœ… RealisticBacktestEngine class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… EnsembleSignalGenerator class loaded\n"
          ]
        }
      ],
      "source": [
        "## Ensemble Strategy: TA + On-Chain + Macro Signal Generator\n",
        "\n",
        "class EnsembleSignalGenerator:\n",
        "    \"\"\"\n",
        "    Multi-source ensemble strategy combining:\n",
        "    - Technical Analysis signals\n",
        "    - On-chain metrics\n",
        "    - Macroeconomic indicators\n",
        "    \n",
        "    Uses weighted voting system with configurable thresholds\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {}\n",
        "        ensemble_cfg = self.config.get('strategies', {}).get('ensemble', {})\n",
        "        \n",
        "        self.weights = ensemble_cfg.get('weights', {\n",
        "            'technical': 0.5,\n",
        "            'onchain': 0.3,\n",
        "            'macro': 0.2\n",
        "        })\n",
        "        \n",
        "        self.min_agreement = ensemble_cfg.get('min_agreement', 2)\n",
        "        \n",
        "    def generate_ta_signal(self, row):\n",
        "        \"\"\"\n",
        "        Generate Technical Analysis signal (-1, 0, 1)\n",
        "        \n",
        "        Rules:\n",
        "        - RSI oversold + BB lower touch = BUY\n",
        "        - RSI overbought + BB upper touch = SELL\n",
        "        - SuperTrend direction\n",
        "        \"\"\"\n",
        "        signals = []\n",
        "        \n",
        "        # RSI signals\n",
        "        if 'RSI' in row:\n",
        "            if row['RSI'] < 30:\n",
        "                signals.append(1)  # Oversold -> Buy\n",
        "            elif row['RSI'] > 70:\n",
        "                signals.append(-1)  # Overbought -> Sell\n",
        "            else:\n",
        "                signals.append(0)\n",
        "        \n",
        "        # Bollinger Bands\n",
        "        if all(k in row for k in ['close', 'BBL_20_2.0', 'BBU_20_2.0', 'BBM_20_2.0']):\n",
        "            if row['close'] < row['BBL_20_2.0']:\n",
        "                signals.append(1)  # Below lower band -> Buy\n",
        "            elif row['close'] > row['BBU_20_2.0']:\n",
        "                signals.append(-1)  # Above upper band -> Sell\n",
        "            else:\n",
        "                signals.append(0)\n",
        "        \n",
        "        # SuperTrend\n",
        "        if 'SUPERTd_10_3.0' in row:\n",
        "            if row['SUPERTd_10_3.0'] == 1:\n",
        "                signals.append(1)  # Uptrend -> Buy\n",
        "            elif row['SUPERTd_10_3.0'] == -1:\n",
        "                signals.append(-1)  # Downtrend -> Sell\n",
        "        \n",
        "        # Aggregate TA signals\n",
        "        if not signals:\n",
        "            return 0\n",
        "        \n",
        "        avg_signal = sum(signals) / len(signals)\n",
        "        if avg_signal > 0.3:\n",
        "            return 1\n",
        "        elif avg_signal < -0.3:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def generate_onchain_signal(self, row, asset='BTC'):\n",
        "        \"\"\"\n",
        "        Generate On-Chain signal (-1, 0, 1)\n",
        "        \n",
        "        Rules:\n",
        "        - Low NVT + Rising MVRV = Bullish\n",
        "        - High NVT + Falling MVRV = Bearish\n",
        "        - Exchange outflows = Bullish (accumulation)\n",
        "        \"\"\"\n",
        "        signals = []\n",
        "        \n",
        "        # NVT signal (lower is better for buying)\n",
        "        nvt_cols = [c for c in row.index if asset in c and 'NVT' in c]\n",
        "        if nvt_cols:\n",
        "            nvt = row[nvt_cols[0]]\n",
        "            if pd.notna(nvt):\n",
        "                if nvt < 40:  # Low NVT = undervalued\n",
        "                    signals.append(1)\n",
        "                elif nvt > 100:  # High NVT = overvalued\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        # MVRV signal\n",
        "        mvrv_cols = [c for c in row.index if asset in c and 'MVRV' in c]\n",
        "        if mvrv_cols:\n",
        "            mvrv = row[mvrv_cols[0]]\n",
        "            if pd.notna(mvrv):\n",
        "                if mvrv < 1.0:  # Undervalued\n",
        "                    signals.append(1)\n",
        "                elif mvrv > 3.0:  # Overvalued\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        # Exchange flow signal\n",
        "        flow_cols = [c for c in row.index if asset in c and 'exchange_flow' in c]\n",
        "        if flow_cols:\n",
        "            flow = row[flow_cols[0]]\n",
        "            if pd.notna(flow):\n",
        "                if flow < 0.8:  # Low exchange activity = accumulation\n",
        "                    signals.append(1)\n",
        "                elif flow > 1.5:  # High exchange activity = distribution\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        if not signals:\n",
        "            return 0\n",
        "        \n",
        "        avg_signal = sum(signals) / len(signals)\n",
        "        if avg_signal > 0.3:\n",
        "            return 1\n",
        "        elif avg_signal < -0.3:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def generate_macro_signal(self, row):\n",
        "        \"\"\"\n",
        "        Generate Macro signal (-1, 0, 1)\n",
        "        \n",
        "        Rules:\n",
        "        - Rising DXY = Risk-off = Bearish for crypto\n",
        "        - Falling DXY = Risk-on = Bullish for crypto\n",
        "        - Rising rates = Bearish\n",
        "        - High inflation = Bullish (store of value narrative)\n",
        "        \"\"\"\n",
        "        signals = []\n",
        "        \n",
        "        # DXY signal (inverse for crypto)\n",
        "        if 'DXY' in row:\n",
        "            dxy = row['DXY']\n",
        "            if pd.notna(dxy):\n",
        "                # Calculate DXY momentum (compare to recent average)\n",
        "                # Simplified: assume we have DXY_ma column\n",
        "                dxy_baseline = 100  # Approximate neutral level\n",
        "                if dxy < dxy_baseline - 2:  # Weak dollar\n",
        "                    signals.append(1)\n",
        "                elif dxy > dxy_baseline + 2:  # Strong dollar\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        # Fed Funds Rate signal\n",
        "        if 'FEDFUNDS' in row:\n",
        "            rate = row['FEDFUNDS']\n",
        "            if pd.notna(rate):\n",
        "                if rate < 1.0:  # Low rates = bullish\n",
        "                    signals.append(1)\n",
        "                elif rate > 4.0:  # High rates = bearish\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        # US10Y signal\n",
        "        if 'DGS10' in row or 'US10Y' in row:\n",
        "            col = 'DGS10' if 'DGS10' in row else 'US10Y'\n",
        "            yield_10y = row[col]\n",
        "            if pd.notna(yield_10y):\n",
        "                if yield_10y < 2.0:  # Low yields = risk-on\n",
        "                    signals.append(1)\n",
        "                elif yield_10y > 4.5:  # High yields = risk-off\n",
        "                    signals.append(-1)\n",
        "                else:\n",
        "                    signals.append(0)\n",
        "        \n",
        "        if not signals:\n",
        "            return 0\n",
        "        \n",
        "        avg_signal = sum(signals) / len(signals)\n",
        "        if avg_signal > 0.2:\n",
        "            return 1\n",
        "        elif avg_signal < -0.2:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def generate_ensemble_signal(self, row, asset='BTC'):\n",
        "        \"\"\"\n",
        "        Generate final ensemble signal by combining all sources\n",
        "        \n",
        "        Returns:\n",
        "            signal: -1 (sell), 0 (neutral), 1 (buy)\n",
        "            confidence: 0.0 to 1.0\n",
        "            components: dict with individual signals\n",
        "        \"\"\"\n",
        "        # Get individual signals\n",
        "        ta_signal = self.generate_ta_signal(row)\n",
        "        onchain_signal = self.generate_onchain_signal(row, asset)\n",
        "        macro_signal = self.generate_macro_signal(row)\n",
        "        \n",
        "        components = {\n",
        "            'technical': ta_signal,\n",
        "            'onchain': onchain_signal,\n",
        "            'macro': macro_signal\n",
        "        }\n",
        "        \n",
        "        # Weighted voting\n",
        "        weighted_score = (\n",
        "            ta_signal * self.weights['technical'] +\n",
        "            onchain_signal * self.weights['onchain'] +\n",
        "            macro_signal * self.weights['macro']\n",
        "        )\n",
        "        \n",
        "        # Count agreements\n",
        "        non_zero_signals = [s for s in [ta_signal, onchain_signal, macro_signal] if s != 0]\n",
        "        agreements = len([s for s in non_zero_signals if (s > 0 and weighted_score > 0) or (s < 0 and weighted_score < 0)])\n",
        "        \n",
        "        # Final decision with confidence\n",
        "        if abs(weighted_score) > 0.3 and agreements >= self.min_agreement:\n",
        "            final_signal = 1 if weighted_score > 0 else -1\n",
        "            confidence = min(abs(weighted_score), 1.0) * (agreements / 3.0)\n",
        "        else:\n",
        "            final_signal = 0\n",
        "            confidence = 0.0\n",
        "        \n",
        "        return final_signal, confidence, components\n",
        "    \n",
        "    def check_ensemble_entry(self, row, asset='BTC', min_confidence=0.4):\n",
        "        \"\"\"\n",
        "        Entry condition for ensemble strategy\n",
        "        \"\"\"\n",
        "        signal, confidence, _ = self.generate_ensemble_signal(row, asset)\n",
        "        return signal == 1 and confidence >= min_confidence\n",
        "    \n",
        "    def check_ensemble_exit(self, row, asset='BTC'):\n",
        "        \"\"\"\n",
        "        Exit condition for ensemble strategy\n",
        "        \"\"\"\n",
        "        signal, confidence, _ = self.generate_ensemble_signal(row, asset)\n",
        "        # Exit on sell signal OR neutral with declining confidence\n",
        "        return signal == -1 or (signal == 0 and confidence < 0.2)\n",
        "\n",
        "print(\"âœ… EnsembleSignalGenerator class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… WalkForwardAnalyzer class loaded\n"
          ]
        }
      ],
      "source": [
        "## Walk-Forward Analysis for Model Validation\n",
        "\n",
        "class WalkForwardAnalyzer:\n",
        "    \"\"\"\n",
        "    Implements walk-forward analysis to prevent overfitting\n",
        "    \n",
        "    Process:\n",
        "    1. Split data into train/test windows\n",
        "    2. Train on past data\n",
        "    3. Test on future data\n",
        "    4. Roll forward and repeat\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {}\n",
        "        wf_cfg = self.config.get('validation', {}).get('walk_forward', {})\n",
        "        \n",
        "        self.train_months = wf_cfg.get('train_period_months', 12)\n",
        "        self.test_months = wf_cfg.get('test_period_months', 3)\n",
        "        self.step_months = wf_cfg.get('step_months', 3)\n",
        "    \n",
        "    def create_windows(self, df):\n",
        "        \"\"\"\n",
        "        Create train/test windows for walk-forward analysis\n",
        "        \n",
        "        Returns:\n",
        "            List of (train_df, test_df, window_id) tuples\n",
        "        \"\"\"\n",
        "        windows = []\n",
        "        \n",
        "        start_date = df.index.min()\n",
        "        end_date = df.index.max()\n",
        "        \n",
        "        current_date = start_date\n",
        "        window_id = 0\n",
        "        \n",
        "        while True:\n",
        "            # Define train period\n",
        "            train_end = current_date + pd.DateOffset(months=self.train_months)\n",
        "            \n",
        "            # Define test period  \n",
        "            test_start = train_end\n",
        "            test_end = test_start + pd.DateOffset(months=self.test_months)\n",
        "            \n",
        "            # Break if we don't have enough data for test period\n",
        "            if test_end > end_date:\n",
        "                break\n",
        "            \n",
        "            # Extract windows\n",
        "            train_df = df[(df.index >= current_date) & (df.index < train_end)]\n",
        "            test_df = df[(df.index >= test_start) & (df.index < test_end)]\n",
        "            \n",
        "            if len(train_df) > 100 and len(test_df) > 20:  # Minimum data requirements\n",
        "                windows.append((train_df, test_df, window_id))\n",
        "                window_id += 1\n",
        "            \n",
        "            # Roll forward\n",
        "            current_date += pd.DateOffset(months=self.step_months)\n",
        "        \n",
        "        print(f\"âœ… Created {len(windows)} walk-forward windows\")\n",
        "        print(f\"   Train period: {self.train_months} months\")\n",
        "        print(f\"   Test period: {self.test_months} months\")\n",
        "        print(f\"   Step size: {self.step_months} months\")\n",
        "        \n",
        "        return windows\n",
        "    \n",
        "    def run_walk_forward_test(self, df, strategy_func, backtest_engine_class, **strategy_params):\n",
        "        \"\"\"\n",
        "        Run walk-forward analysis on a strategy\n",
        "        \n",
        "        Args:\n",
        "            df: Full dataset\n",
        "            strategy_func: Function that returns (entry_func, exit_func, params)\n",
        "            backtest_engine_class: Backtest engine to use\n",
        "            **strategy_params: Parameters for strategy\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame with results for each window\n",
        "        \"\"\"\n",
        "        windows = self.create_windows(df)\n",
        "        results = []\n",
        "        \n",
        "        for train_df, test_df, window_id in windows:\n",
        "            print(f\"\\nðŸ“Š Window {window_id + 1}/{len(windows)}\")\n",
        "            print(f\"   Train: {train_df.index.min().date()} to {train_df.index.max().date()} ({len(train_df)} bars)\")\n",
        "            print(f\"   Test:  {test_df.index.min().date()} to {test_df.index.max().date()} ({len(test_df)} bars)\")\n",
        "            \n",
        "            # Get strategy configuration (could optimize on train_df here)\n",
        "            entry_func, exit_func, params = strategy_func(train_df, **strategy_params)\n",
        "            \n",
        "            # Test on out-of-sample data\n",
        "            engine = backtest_engine_class(config=self.config)\n",
        "            result = engine.run(test_df, entry_func, exit_func, **params)\n",
        "            \n",
        "            # Store results\n",
        "            window_result = {\n",
        "                'window_id': window_id,\n",
        "                'train_start': train_df.index.min(),\n",
        "                'train_end': train_df.index.max(),\n",
        "                'test_start': test_df.index.min(),\n",
        "                'test_end': test_df.index.max(),\n",
        "                **result['metrics']\n",
        "            }\n",
        "            \n",
        "            results.append(window_result)\n",
        "            \n",
        "            print(f\"   Result: PnL={result['metrics'].get('total_return_pct', 0):.2f}%, Trades={result['metrics'].get('total_trades', 0)}\")\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        \n",
        "        # Summary statistics\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"WALK-FORWARD ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Total windows: {len(results_df)}\")\n",
        "        print(f\"Avg PnL: {results_df['total_return_pct'].mean():.2f}%\")\n",
        "        print(f\"Median PnL: {results_df['total_return_pct'].median():.2f}%\")\n",
        "        print(f\"Win rate (profitable windows): {(results_df['total_return_pct'] > 0).sum() / len(results_df) * 100:.1f}%\")\n",
        "        print(f\"Avg Sharpe: {results_df['sharpe_ratio'].mean():.3f}\")\n",
        "        print(f\"Avg Max DD: {results_df['max_drawdown_pct'].mean():.2f}%\")\n",
        "        \n",
        "        return results_df\n",
        "\n",
        "print(\"âœ… WalkForwardAnalyzer class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… December2025Predictor class loaded\n"
          ]
        }
      ],
      "source": [
        "## December 2025 Predictions & Reality Comparison\n",
        "\n",
        "class December2025Predictor:\n",
        "    \"\"\"\n",
        "    Generate predictions for December 2025 and compare with actual results\n",
        "    \n",
        "    Methods:\n",
        "    - Simple momentum extrapolation\n",
        "    - Strategy-based signal projection\n",
        "    - Confidence intervals\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.december_start = pd.Timestamp('2025-12-01')\n",
        "        self.december_end = pd.Timestamp('2025-12-31')\n",
        "    \n",
        "    def generate_predictions(self, df, strategy, asset='BTC'):\n",
        "        \"\"\"\n",
        "        Generate December 2025 predictions based on historical patterns\n",
        "        \n",
        "        Args:\n",
        "            df: Historical data up to Nov 2025\n",
        "            strategy: Strategy object (ensemble or other)\n",
        "            asset: Asset symbol\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame with predictions and confidence intervals\n",
        "        \"\"\"\n",
        "        print(f\"\\nðŸ”® Generating December 2025 Predictions for {asset}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Get last known data point\n",
        "        last_date = df.index.max()\n",
        "        last_price = df['close'].iloc[-1]\n",
        "        \n",
        "        print(f\"Last known data: {last_date.date()}, Price: ${last_price:,.2f}\")\n",
        "        \n",
        "        # Calculate recent momentum and volatility\n",
        "        returns_30d = df['close'].pct_change().tail(30)\n",
        "        avg_return = returns_30d.mean()\n",
        "        volatility = returns_30d.std()\n",
        "        \n",
        "        print(f\"30-day avg return: {avg_return*100:.3f}%\")\n",
        "        print(f\"30-day volatility: {volatility*100:.2f}%\")\n",
        "        \n",
        "        # Generate December dates\n",
        "        december_dates = pd.date_range(self.december_start, self.december_end, freq='D')\n",
        "        \n",
        "        predictions = []\n",
        "        \n",
        "        for i, date in enumerate(december_dates):\n",
        "            days_ahead = (date - last_date).days\n",
        "            \n",
        "            # Simple momentum projection\n",
        "            expected_return = avg_return * days_ahead\n",
        "            expected_price = last_price * (1 + expected_return)\n",
        "            \n",
        "            # Confidence intervals (1Ïƒ and 2Ïƒ)\n",
        "            std_dev = volatility * np.sqrt(days_ahead)\n",
        "            \n",
        "            ci_68_lower = last_price * (1 + expected_return - std_dev)\n",
        "            ci_68_upper = last_price * (1 + expected_return + std_dev)\n",
        "            ci_95_lower = last_price * (1 + expected_return - 2*std_dev)\n",
        "            ci_95_upper = last_price * (1 + expected_return + 2*std_dev)\n",
        "            \n",
        "            # Strategy signal projection (if available)\n",
        "            strategy_signal = 0\n",
        "            if hasattr(strategy, 'generate_ensemble_signal') and not df.empty:\n",
        "                last_row = df.iloc[-1]\n",
        "                signal, confidence, _ = strategy.generate_ensemble_signal(last_row, asset)\n",
        "                strategy_signal = signal\n",
        "            \n",
        "            predictions.append({\n",
        "                'date': date,\n",
        "                'days_ahead': days_ahead,\n",
        "                'predicted_price': expected_price,\n",
        "                'ci_68_lower': ci_68_lower,\n",
        "                'ci_68_upper': ci_68_upper,\n",
        "                'ci_95_lower': ci_95_lower,\n",
        "                'ci_95_upper': ci_95_upper,\n",
        "                'strategy_signal': strategy_signal,\n",
        "                'actual_price': None  # Will be filled when real data available\n",
        "            })\n",
        "        \n",
        "        pred_df = pd.DataFrame(predictions)\n",
        "        pred_df.set_index('date', inplace=True)\n",
        "        \n",
        "        print(f\"\\nâœ… Generated {len(pred_df)} daily predictions\")\n",
        "        print(f\"   Predicted range: ${pred_df['predicted_price'].min():,.0f} - ${pred_df['predicted_price'].max():,.0f}\")\n",
        "        print(f\"   95% CI range: ${pred_df['ci_95_lower'].min():,.0f} - ${pred_df['ci_95_upper'].max():,.0f}\")\n",
        "        \n",
        "        return pred_df\n",
        "    \n",
        "    def compare_with_actual(self, predictions_df, actual_df):\n",
        "        \"\"\"\n",
        "        Compare predictions with actual December 2025 data\n",
        "        \n",
        "        Args:\n",
        "            predictions_df: DataFrame with predictions\n",
        "            actual_df: DataFrame with actual price data\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame with comparison metrics\n",
        "        \"\"\"\n",
        "        print(\"\\nðŸ“Š Comparing Predictions vs Actual\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Merge predictions with actuals\n",
        "        comparison = predictions_df.copy()\n",
        "        \n",
        "        for date in comparison.index:\n",
        "            if date in actual_df.index:\n",
        "                comparison.loc[date, 'actual_price'] = actual_df.loc[date, 'close']\n",
        "        \n",
        "        # Calculate errors\n",
        "        valid_comparisons = comparison[comparison['actual_price'].notna()]\n",
        "        \n",
        "        if len(valid_comparisons) == 0:\n",
        "            print(\"âš ï¸  No actual data available yet for December 2025\")\n",
        "            return comparison\n",
        "        \n",
        "        valid_comparisons['prediction_error'] = (\n",
        "            (valid_comparisons['actual_price'] - valid_comparisons['predicted_price']) / \n",
        "            valid_comparisons['actual_price'] * 100\n",
        "        )\n",
        "        \n",
        "        valid_comparisons['within_ci_68'] = (\n",
        "            (valid_comparisons['actual_price'] >= valid_comparisons['ci_68_lower']) &\n",
        "            (valid_comparisons['actual_price'] <= valid_comparisons['ci_68_upper'])\n",
        "        )\n",
        "        \n",
        "        valid_comparisons['within_ci_95'] = (\n",
        "            (valid_comparisons['actual_price'] >= valid_comparisons['ci_95_lower']) &\n",
        "            (valid_comparisons['actual_price'] <= valid_comparisons['ci_95_upper'])\n",
        "        )\n",
        "        \n",
        "        # Summary metrics\n",
        "        mae = valid_comparisons['prediction_error'].abs().mean()\n",
        "        rmse = np.sqrt((valid_comparisons['prediction_error'] ** 2).mean())\n",
        "        ci_68_coverage = valid_comparisons['within_ci_68'].mean() * 100\n",
        "        ci_95_coverage = valid_comparisons['within_ci_95'].mean() * 100\n",
        "        \n",
        "        print(f\"Comparison metrics ({len(valid_comparisons)} days):\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.2f}%\")\n",
        "        print(f\"  Root Mean Square Error: {rmse:.2f}%\")\n",
        "        print(f\"  68% CI Coverage: {ci_68_coverage:.1f}% (expected ~68%)\")\n",
        "        print(f\"  95% CI Coverage: {ci_95_coverage:.1f}% (expected ~95%)\")\n",
        "        \n",
        "        return comparison\n",
        "    \n",
        "    def plot_predictions_vs_actual(self, comparison_df, asset='BTC', save_path='december_2025_comparison.png'):\n",
        "        \"\"\"\n",
        "        Plot predictions vs actual data with confidence intervals\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        \n",
        "        # Plot predictions\n",
        "        ax.plot(comparison_df.index, comparison_df['predicted_price'], \n",
        "                label='Predicted Price', color='blue', linewidth=2, linestyle='--')\n",
        "        \n",
        "        # Plot confidence intervals\n",
        "        ax.fill_between(comparison_df.index, \n",
        "                        comparison_df['ci_68_lower'], \n",
        "                        comparison_df['ci_68_upper'],\n",
        "                        alpha=0.3, color='blue', label='68% Confidence Interval')\n",
        "        \n",
        "        ax.fill_between(comparison_df.index, \n",
        "                        comparison_df['ci_95_lower'], \n",
        "                        comparison_df['ci_95_upper'],\n",
        "                        alpha=0.15, color='blue', label='95% Confidence Interval')\n",
        "        \n",
        "        # Plot actual prices if available\n",
        "        actual_data = comparison_df[comparison_df['actual_price'].notna()]\n",
        "        if len(actual_data) > 0:\n",
        "            ax.plot(actual_data.index, actual_data['actual_price'],\n",
        "                   label='Actual Price', color='red', linewidth=2, marker='o', markersize=4)\n",
        "        \n",
        "        ax.set_title(f'{asset} December 2025: Predictions vs Actual', fontsize=16, fontweight='bold')\n",
        "        ax.set_xlabel('Date', fontsize=12)\n",
        "        ax.set_ylabel('Price (USD)', fontsize=12)\n",
        "        ax.legend(loc='best', fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"âœ… Chart saved to {save_path}\")\n",
        "\n",
        "print(\"âœ… December2025Predictor class loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Bias control & validation functions loaded\n"
          ]
        }
      ],
      "source": [
        "## Bias Control & Validation Functions\n",
        "\n",
        "def validate_no_lookahead_bias(df, verbose=True):\n",
        "    \"\"\"\n",
        "    Validates that the DataFrame doesn't contain look-ahead bias\n",
        "    \n",
        "    Checks:\n",
        "    1. Chikou Span uses backward shift (not forward)\n",
        "    2. No forward-shifted columns used in strategy\n",
        "    3. Senkou Spans don't use future data\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to validate\n",
        "        verbose: Print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        True if no bias detected, raises AssertionError otherwise\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ” LOOK-AHEAD BIAS VALIDATION\")\n",
        "        print(\"=\"*70)\n",
        "    \n",
        "    checks_passed = []\n",
        "    \n",
        "    # Check 1: Chikou uses backward shift\n",
        "    if 'close_26_back' in df.columns:\n",
        "        # Last 26 rows should have NaN (proof of backward shift)\n",
        "        chikou_nans = df['close_26_back'].isna().tail(26).sum()\n",
        "        if chikou_nans == 26:\n",
        "            checks_passed.append(\"âœ… Chikou Span: Backward shift validated (last 26 rows are NaN)\")\n",
        "        else:\n",
        "            raise AssertionError(f\"âŒ Chikou leak detected! Expected 26 NaN at end, found {chikou_nans}\")\n",
        "    \n",
        "    # Check 2: No forward-shifted columns in strategy\n",
        "    fwd_cols = [c for c in df.columns if c.endswith('_fwd')]\n",
        "    if len(fwd_cols) == 0:\n",
        "        checks_passed.append(\"âœ… No forward-shifted columns found in strategy data\")\n",
        "    else:\n",
        "        raise AssertionError(f\"âŒ Forward-shifted columns detected (plot-only): {fwd_cols}\")\n",
        "    \n",
        "    # Check 3: Senkou Spans are not shifted\n",
        "    if 'senkou_span_a' in df.columns and 'senkou_span_b' in df.columns:\n",
        "        # First 52 rows should have NaN (warmup period, not future data)\n",
        "        span_a_start_nans = df['senkou_span_a'].isna().head(52).sum()\n",
        "        span_b_start_nans = df['senkou_span_b'].isna().head(52).sum()\n",
        "        \n",
        "        if span_a_start_nans > 0 and span_b_start_nans > 0:\n",
        "            checks_passed.append(f\"âœ… Senkou Spans: No forward shift (warmup NaNs: A={span_a_start_nans}, B={span_b_start_nans})\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Warning: Senkou Spans may not have expected warmup period\")\n",
        "    \n",
        "    # Check 4: Index is sorted (time series integrity)\n",
        "    if df.index.is_monotonic_increasing:\n",
        "        checks_passed.append(\"âœ… Time index is properly sorted (no future data mixed)\")\n",
        "    else:\n",
        "        raise AssertionError(\"âŒ Time index not sorted! Data may be shuffled\")\n",
        "    \n",
        "    if verbose:\n",
        "        for check in checks_passed:\n",
        "            print(check)\n",
        "        print(\"=\"*70)\n",
        "        print(\"âœ… ALL LOOK-AHEAD BIAS CHECKS PASSED\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "def sanity_check_next_bar_execution(df, entry_func, exit_func, entry_params={}, exit_params={}, sample_size=None):\n",
        "    \"\"\"\n",
        "    Sanity check: Compare same-bar vs next-bar execution\n",
        "    \n",
        "    Large differences suggest potential look-ahead bias in signal generation\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to test\n",
        "        entry_func: Entry function\n",
        "        exit_func: Exit function\n",
        "        entry_params: Entry parameters\n",
        "        exit_params: Exit parameters\n",
        "        sample_size: If specified, test on last N bars only\n",
        "    \n",
        "    Returns:\n",
        "        Dict with comparison results\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ” NEXT-BAR EXECUTION SANITY CHECK\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Use sample if specified\n",
        "    test_df = df.tail(sample_size) if sample_size else df\n",
        "    \n",
        "    # Test with same-bar execution (potential bias)\n",
        "    print(\"Testing same-bar execution (may contain bias)...\")\n",
        "    pnl_same, wr_same, trades_same, _ = run_backtest(\n",
        "        test_df.copy(), entry_func, exit_func, entry_params, exit_params, \n",
        "        enforce_profitable_exit=False\n",
        "    )\n",
        "    \n",
        "    # Note: We need to add execute_next_bar parameter to run_backtest\n",
        "    # For now, just report that this check requires updated run_backtest function\n",
        "    \n",
        "    print(f\"  Same-bar: PnL={pnl_same:.2f}%, WinRate={wr_same:.1f}%, Trades={trades_same}\")\n",
        "    print(\"\\nâš ï¸  Note: Full next-bar execution test requires run_backtest update\")\n",
        "    print(\"   Current results use close price (conservative)\")\n",
        "    \n",
        "    result = {\n",
        "        'same_bar_pnl': pnl_same,\n",
        "        'same_bar_win_rate': wr_same,\n",
        "        'same_bar_trades': trades_same,\n",
        "        'bias_risk': 'LOW' if pnl_same < 50 else 'MEDIUM'\n",
        "    }\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    return result\n",
        "\n",
        "def validate_ichimoku_integrity(df, verbose=True):\n",
        "    \"\"\"\n",
        "    Specific validation for Ichimoku Cloud integrity\n",
        "    \n",
        "    Ensures:\n",
        "    1. No forward shifting used\n",
        "    2. Chikou uses correct backward reference\n",
        "    3. Cloud calculations use historical data only\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with Ichimoku indicators\n",
        "        verbose: Print detailed results\n",
        "    \n",
        "    Returns:\n",
        "        True if valid, raises AssertionError otherwise\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ” ICHIMOKU INTEGRITY VALIDATION\")\n",
        "        print(\"=\"*70)\n",
        "    \n",
        "    required_cols = ['tenkan_sen', 'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'close_26_back']\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "    \n",
        "    if missing:\n",
        "        raise AssertionError(f\"âŒ Missing Ichimoku columns: {missing}\")\n",
        "    \n",
        "    checks = []\n",
        "    \n",
        "    # Check 1: Tenkan uses 9-period lookback (not forward)\n",
        "    # Should have NaN in first ~9 rows\n",
        "    tenkan_start_nans = df['tenkan_sen'].isna().head(9).sum()\n",
        "    if tenkan_start_nans > 0:\n",
        "        checks.append(f\"âœ… Tenkan-sen: Backward-looking ({tenkan_start_nans} warmup NaNs)\")\n",
        "    \n",
        "    # Check 2: Kijun uses 26-period lookback\n",
        "    kijun_start_nans = df['kijun_sen'].isna().head(26).sum()\n",
        "    if kijun_start_nans > 0:\n",
        "        checks.append(f\"âœ… Kijun-sen: Backward-looking ({kijun_start_nans} warmup NaNs)\")\n",
        "    \n",
        "    # Check 3: Senkou Span B uses 52-period lookback\n",
        "    span_b_start_nans = df['senkou_span_b'].isna().head(52).sum()\n",
        "    if span_b_start_nans > 0:\n",
        "        checks.append(f\"âœ… Senkou Span B: Backward-looking ({span_b_start_nans} warmup NaNs)\")\n",
        "    \n",
        "    # Check 4: Chikou backward shift creates NaN at END (not beginning)\n",
        "    chikou_end_nans = df['close_26_back'].isna().tail(26).sum()\n",
        "    chikou_start_nans = df['close_26_back'].isna().head(26).sum()\n",
        "    \n",
        "    if chikou_end_nans == 26 and chikou_start_nans == 0:\n",
        "        checks.append(f\"âœ… Chikou Span: Correct backward shift (NaN at end, not start)\")\n",
        "    else:\n",
        "        raise AssertionError(\n",
        "            f\"âŒ Chikou shift error! End NaNs: {chikou_end_nans}/26, Start NaNs: {chikou_start_nans}\"\n",
        "        )\n",
        "    \n",
        "    # Check 5: No _fwd columns (those are plot-only)\n",
        "    fwd_cols = [c for c in df.columns if '_fwd' in c]\n",
        "    if len(fwd_cols) == 0:\n",
        "        checks.append(\"âœ… No forward-shifted columns in strategy data\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Found {len(fwd_cols)} forward-shifted columns (ensure these are NOT used in entry/exit logic)\")\n",
        "    \n",
        "    if verbose:\n",
        "        for check in checks:\n",
        "            print(check)\n",
        "        print(\"=\"*70)\n",
        "        print(\"âœ… ICHIMOKU INTEGRITY VALIDATED - NO FUTURE LEAK\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "print(\"âœ… Bias control & validation functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Validation suite ready. Use: run_full_validation_suite(df)\n"
          ]
        }
      ],
      "source": [
        "# Validation Example - Run before backtesting to ensure data integrity\n",
        "\n",
        "def run_full_validation_suite(df, strategy_type='ichimoku'):\n",
        "    \"\"\"\n",
        "    Run comprehensive validation checks before backtesting\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with all indicators\n",
        "        strategy_type: 'ichimoku' or 'general'\n",
        "    \n",
        "    Returns:\n",
        "        True if all checks pass\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ðŸ”¬ RUNNING COMPREHENSIVE VALIDATION SUITE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    try:\n",
        "        # General look-ahead bias checks\n",
        "        validate_no_lookahead_bias(df, verbose=True)\n",
        "        \n",
        "        # Ichimoku-specific checks\n",
        "        if strategy_type == 'ichimoku' or 'tenkan_sen' in df.columns:\n",
        "            validate_ichimoku_integrity(df, verbose=True)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"âœ… ALL VALIDATION CHECKS PASSED - DATA IS SAFE FOR BACKTESTING\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except AssertionError as e:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"âŒ VALIDATION FAILED\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"\\nâš ï¸  DO NOT PROCEED WITH BACKTESTING - FIX DATA ISSUES FIRST\")\n",
        "        return False\n",
        "\n",
        "print(\"âœ… Validation suite ready. Use: run_full_validation_suite(df)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_rsi_entry(row, threshold):\n",
        "    return row['RSI'] < threshold\n",
        "\n",
        "def check_bb_entry(row, prev_row):\n",
        "    # Close[t-1] < BB_Lower[t-1] and Close[t] > BB_Lower[t] (Reversal)\n",
        "    # Assuming column names from pandas_ta: BBL_20_2.0\n",
        "    return prev_row['close'] < prev_row['BBL_20_2.0'] and row['close'] > row['BBL_20_2.0']\n",
        "\n",
        "def check_kama_supertrend(row):\n",
        "    # Close < KAMA and SuperTrend == SELL (-1)\n",
        "    # SuperTrend direction column is usually SUPERTd_10_3.0 (1 for Buy, -1 for Sell)\n",
        "    return row['close'] < row['KAMA'] and row['SUPERTd_10_3.0'] == -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_rsi_exit(row, threshold):\n",
        "    return row['RSI'] > threshold\n",
        "\n",
        "def check_supertrend_exit(row):\n",
        "    # SuperTrend == BUY (1)\n",
        "    return row['SUPERTd_10_3.0'] == 1\n",
        "\n",
        "def check_bb_exit_A(row):\n",
        "    # BB_Middle < Close < BB_Upper\n",
        "    return row['BBM_20_2.0'] < row['close'] < row['BBU_20_2.0']\n",
        "\n",
        "def check_bb_exit_B(row, near_ratio=0.7):\n",
        "    \"\"\"\n",
        "    BB Exit B with near_ratio parameter:\n",
        "    - Close must be between BB_Lower and BB_Middle\n",
        "    - near_ratio filters by proximity to middle band\n",
        "    - ratio = (close - lower) / (middle - lower)\n",
        "    - Higher near_ratio means closer to middle band\n",
        "    \"\"\"\n",
        "    low = row['BBL_20_2.0']\n",
        "    mid = row['BBM_20_2.0']\n",
        "    c = row['close']\n",
        "    \n",
        "    # Check if in range\n",
        "    if not (low < c < mid):\n",
        "        return False\n",
        "    \n",
        "    # Calculate ratio (0 = at lower, 1 = at middle)\n",
        "    width = max(1e-9, (mid - low))\n",
        "    ratio = (c - low) / width\n",
        "    \n",
        "    # Return True if close is at or above the threshold\n",
        "    return ratio >= near_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Grid Search...\n",
            "======================================================================\n",
            "\n",
            "Processing BTCUSDT_1d...\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 1233 rows\n",
            "\n",
            "Processing BTCUSDT_1h...\n",
            "  Original data: 34348 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 34149 rows\n",
            "\n",
            "Processing BTCUSDT_4h...\n",
            "  Original data: 8588 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 8389 rows\n",
            "\n",
            "Processing ETHUSDT_1d...\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 1233 rows\n",
            "\n",
            "Processing ETHUSDT_1h...\n",
            "  Original data: 34348 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 34149 rows\n",
            "\n",
            "Processing ETHUSDT_4h...\n",
            "  Original data: 8588 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after indicators: 8389 rows\n",
            "Grid Search Results saved to 'grid_search_results.csv'\n",
            "\n",
            "======================================================================\n",
            "Saving best combination and detailed trade list...\n",
            "Best combo saved to 'grid_search_best.csv'\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "Best combo trade list saved to 'grid_best_trades.csv' (18 trades)\n",
            "\n",
            "======================================================================\n",
            "Grid Search Complete. Top 5 Results:\n",
            "      Symbol_TF            Entry  Entry_Param       Exit  Exit_Param    PnL%  \\\n",
            "284  ETHUSDT_1d              RSI         40.0  BB_Exit_B        0.85  431.04   \n",
            "50   BTCUSDT_1d              RSI         40.0  BB_Exit_B        0.85  243.27   \n",
            "49   BTCUSDT_1d              RSI         40.0  BB_Exit_B        0.80  237.67   \n",
            "70   BTCUSDT_1d  KAMA_SuperTrend          NaN  BB_Exit_A         NaN  235.39   \n",
            "285  ETHUSDT_1d              RSI         40.0  BB_Exit_B        0.90  219.96   \n",
            "\n",
            "     Win_Rate%  Trades  \n",
            "284      94.44      18  \n",
            "50       94.74      19  \n",
            "49       94.74      19  \n",
            "70       97.37      38  \n",
            "285      92.31      13  \n"
          ]
        }
      ],
      "source": [
        "# Grid Search Configuration\n",
        "rsi_entry_thresholds = [20, 25, 30, 40]\n",
        "rsi_exit_thresholds = [40, 45, 50, 60]\n",
        "near_ratio_candidates = [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
        "\n",
        "entry_strategies = [\n",
        "    ('RSI', check_rsi_entry, rsi_entry_thresholds),\n",
        "    ('BB_Reversal', check_bb_entry, [None]),\n",
        "    ('KAMA_SuperTrend', check_kama_supertrend, [None])\n",
        "]\n",
        "\n",
        "exit_strategies = [\n",
        "    ('RSI', check_rsi_exit, rsi_exit_thresholds),\n",
        "    ('SuperTrend', check_supertrend_exit, [None]),\n",
        "    ('BB_Exit_A', check_bb_exit_A, [None]),\n",
        "    ('BB_Exit_B', check_bb_exit_B, near_ratio_candidates)\n",
        "]\n",
        "\n",
        "# Create function maps for later use\n",
        "entry_func_map = {name: fn for (name, fn, _) in entry_strategies}\n",
        "exit_func_map = {name: fn for (name, fn, _) in exit_strategies}\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_grid_search():\n",
        "    print(\"Starting Grid Search...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load Data\n",
        "    data_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv') and 'macro' not in f]\n",
        "    \n",
        "    for file in data_files:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        print(f\"\\nProcessing {symbol_tf}...\")\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        print(f\"  Original data: {len(df)} rows\")\n",
        "        \n",
        "        # Calculate Indicators (dropna is done inside this function now)\n",
        "        df = calculate_indicators(df)\n",
        "        \n",
        "        print(f\"  Data after indicators: {len(df)} rows\")\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"  WARNING: {symbol_tf} has 0 rows after indicator calculation!\")\n",
        "            continue\n",
        "        \n",
        "        # Iterate Combinations\n",
        "        for entry_name, entry_func, entry_params_list in entry_strategies:\n",
        "            for exit_name, exit_func, exit_params_list in exit_strategies:\n",
        "                \n",
        "                # Handle Parameter Expansion\n",
        "                for entry_param in entry_params_list:\n",
        "                    for exit_param in exit_params_list:\n",
        "                        \n",
        "                        # Prepare Params dicts\n",
        "                        e_p = {'threshold': entry_param} if entry_name == 'RSI' else {}\n",
        "                        \n",
        "                        if exit_name == 'RSI':\n",
        "                            x_p = {'threshold': exit_param}\n",
        "                        elif exit_name == 'BB_Exit_B':\n",
        "                            x_p = {'near_ratio': exit_param}\n",
        "                        else:\n",
        "                            x_p = {}\n",
        "                        \n",
        "                        # Run Backtest with profitable exit enforcement (TC1 requirement)\n",
        "                        pnl, win_rate, trades_cnt, _ = run_backtest(\n",
        "                            df, entry_func, exit_func, e_p, x_p, enforce_profitable_exit=True\n",
        "                        )\n",
        "                        \n",
        "                        results.append({\n",
        "                            'Symbol_TF': symbol_tf,\n",
        "                            'Entry': entry_name,\n",
        "                            'Entry_Param': entry_param,\n",
        "                            'Exit': exit_name,\n",
        "                            'Exit_Param': exit_param,\n",
        "                            'PnL%': round(pnl, 2),\n",
        "                            'Win_Rate%': round(win_rate, 2),\n",
        "                            'Trades': trades_cnt\n",
        "                        })\n",
        "                        \n",
        "    # Save Results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    # Save to CSV\n",
        "    results_df.to_csv('grid_search_results.csv', index=False)\n",
        "    print(\"Grid Search Results saved to 'grid_search_results.csv'\")\n",
        "    \n",
        "    # Save Best Combo and its Trade List\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Saving best combination and detailed trade list...\")\n",
        "    \n",
        "    best = results_df.iloc[0]\n",
        "    best.to_frame().T.to_csv('grid_search_best.csv', index=False)\n",
        "    print(f\"Best combo saved to 'grid_search_best.csv'\")\n",
        "    \n",
        "    # Re-run best strategy to get detailed trades\n",
        "    best_entry_func = entry_func_map[best['Entry']]\n",
        "    best_exit_func = exit_func_map[best['Exit']]\n",
        "    \n",
        "    # Load the best symbol's data\n",
        "    file = f\"{best['Symbol_TF']}.csv\"\n",
        "    df_best = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "    df_best['timestamp'] = pd.to_datetime(df_best['timestamp'])\n",
        "    df_best.set_index('timestamp', inplace=True)\n",
        "    df_best = calculate_indicators(df_best)\n",
        "    \n",
        "    # Prepare params\n",
        "    best_e_p = {'threshold': best['Entry_Param']} if best['Entry'] == 'RSI' else {}\n",
        "    if best['Exit'] == 'RSI':\n",
        "        best_x_p = {'threshold': best['Exit_Param']}\n",
        "    elif best['Exit'] == 'BB_Exit_B':\n",
        "        best_x_p = {'near_ratio': best['Exit_Param']}\n",
        "    else:\n",
        "        best_x_p = {}\n",
        "    \n",
        "    # Get detailed trades\n",
        "    _, _, _, best_trades = run_backtest(\n",
        "        df_best, best_entry_func, best_exit_func, best_e_p, best_x_p, enforce_profitable_exit=True\n",
        "    )\n",
        "    \n",
        "    trades_df = pd.DataFrame(best_trades)\n",
        "    trades_df.to_csv('grid_best_trades.csv', index=False)\n",
        "    print(f\"Best combo trade list saved to 'grid_best_trades.csv' ({len(best_trades)} trades)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Grid Search Complete. Top 5 Results:\")\n",
        "    print(results_df.head())\n",
        "    return results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_df = run_grid_search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "def calculate_ichimoku(df, shift_forward_for_plot=False):\n",
        "    \"\"\"\n",
        "    Calculates Ichimoku Cloud components with leak control.\n",
        "    \n",
        "    IMPORTANT: Senkou Spans are NOT shifted forward for backtesting to prevent look-ahead bias.\n",
        "    Forward shifting is ONLY for visualization if explicitly requested.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with OHLCV data\n",
        "        shift_forward_for_plot: If True, creates _fwd columns for plotting only (NOT for strategy)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with Ichimoku components\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Tenkan-sen (Conversion Line): (9-period high + 9-period low) / 2\n",
        "    high_9 = df['high'].rolling(window=9).max()\n",
        "    low_9 = df['low'].rolling(window=9).min()\n",
        "    df['tenkan_sen'] = (high_9 + low_9) / 2\n",
        "\n",
        "    # Kijun-sen (Base Line): (26-period high + 26-period low) / 2\n",
        "    high_26 = df['high'].rolling(window=26).max()\n",
        "    low_26 = df['low'].rolling(window=26).min()\n",
        "    df['kijun_sen'] = (high_26 + low_26) / 2\n",
        "\n",
        "    # Senkou Span A (Leading Span A): (Tenkan + Kijun) / 2\n",
        "    # CRITICAL: NOT shifted forward for backtesting (prevents future leak)\n",
        "    df['senkou_span_a'] = (df['tenkan_sen'] + df['kijun_sen']) / 2\n",
        "\n",
        "    # Senkou Span B (Leading Span B): (52-period high + 52-period low) / 2\n",
        "    high_52 = df['high'].rolling(window=52).max()\n",
        "    low_52 = df['low'].rolling(window=52).min()\n",
        "    df['senkou_span_b'] = (high_52 + low_52) / 2\n",
        "\n",
        "    # Chikou Span: Compare current close with close 26 periods ago\n",
        "    # This is backward-looking (safe for backtesting)\n",
        "    df['close_26_back'] = df['close'].shift(26)\n",
        "    \n",
        "    # Optional: Create forward-shifted columns ONLY for plotting\n",
        "    if shift_forward_for_plot:\n",
        "        df['senkou_span_a_fwd'] = df['senkou_span_a'].shift(26)\n",
        "        df['senkou_span_b_fwd'] = df['senkou_span_b'].shift(26)\n",
        "        print(\"âš ï¸  Forward-shifted Senkou columns created for plotting ONLY (_fwd suffix)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def identify_market_structure(df, lookback=20):\n",
        "    \"\"\"\n",
        "    Simplified Market Structure identification using rolling highs/lows.\n",
        "    - Uptrend (1): Price making Higher Highs and Higher Lows\n",
        "    - Downtrend (-1): Price making Lower Highs and Lower Lows  \n",
        "    - Neutral (0): Mixed structure\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Find rolling highest high and lowest low\n",
        "    df['rolling_high'] = df['high'].rolling(window=lookback).max()\n",
        "    df['rolling_low'] = df['low'].rolling(window=lookback).min()\n",
        "    \n",
        "    # Previous rolling values for comparison\n",
        "    df['prev_rolling_high'] = df['rolling_high'].shift(lookback)\n",
        "    df['prev_rolling_low'] = df['rolling_low'].shift(lookback)\n",
        "    \n",
        "    # Trend identification\n",
        "    # Uptrend: Current high > Previous period high AND Current low > Previous period low\n",
        "    # Downtrend: Current high < Previous period high AND Current low < Previous period low\n",
        "    \n",
        "    conditions_up = (df['rolling_high'] > df['prev_rolling_high']) & (df['rolling_low'] > df['prev_rolling_low'])\n",
        "    conditions_down = (df['rolling_high'] < df['prev_rolling_high']) & (df['rolling_low'] < df['prev_rolling_low'])\n",
        "    \n",
        "    df['trend'] = 0  # Default neutral\n",
        "    df.loc[conditions_up, 'trend'] = 1\n",
        "    df.loc[conditions_down, 'trend'] = -1\n",
        "    \n",
        "    # Drop helper columns\n",
        "    df.drop(columns=['rolling_high', 'rolling_low', 'prev_rolling_high', 'prev_rolling_low'], inplace=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def check_ichimoku_entry(row):\n",
        "    \"\"\"\n",
        "    Entry conditions:\n",
        "    - Tenkan-sen > Kijun-sen (bullish TK cross)\n",
        "    - Price is above the cloud (Close > both Span A and Span B)\n",
        "    - Chikou Span > Price (Current close > close 26 periods ago)\n",
        "    - Market structure is Uptrend (trend=1)\n",
        "    \"\"\"\n",
        "    tk_bullish = row['tenkan_sen'] > row['kijun_sen']\n",
        "    above_cloud = (row['close'] > row['senkou_span_a']) and (row['close'] > row['senkou_span_b'])\n",
        "    uptrend = row['trend'] == 1\n",
        "    \n",
        "    # Chikou filter: current close should be above close from 26 periods ago\n",
        "    chikou_ok = pd.notna(row.get('close_26_back')) and (row['close'] > row['close_26_back'])\n",
        "    \n",
        "    return tk_bullish and above_cloud and uptrend and chikou_ok\n",
        "\n",
        "def label_market_structure(df, left=3, right=3):\n",
        "    \"\"\"\n",
        "    Labels market structure with HH (Higher High), HL (Higher Low), \n",
        "    LH (Lower High), LL (Lower Low) at pivot points.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'high' and 'low' columns\n",
        "        left: Number of bars to the left for pivot detection\n",
        "        right: Number of bars to the right for pivot detection (not used in argrelextrema)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with 'ms_label' column containing structure labels\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    highs = d['high'].values\n",
        "    lows = d['low'].values\n",
        "    \n",
        "    # Find pivot highs and lows using scipy\n",
        "    piv_hi_idx = argrelextrema(highs, np.greater, order=left)[0]\n",
        "    piv_lo_idx = argrelextrema(lows, np.less, order=left)[0]\n",
        "    \n",
        "    # Initialize ms_label column\n",
        "    d['ms_label'] = np.nan\n",
        "    d['ms_label'] = d['ms_label'].astype('object')\n",
        "    \n",
        "    last_hi = None\n",
        "    last_lo = None\n",
        "    \n",
        "    # Label highs\n",
        "    for i in piv_hi_idx:\n",
        "        if last_hi is None:\n",
        "            d.iloc[i, d.columns.get_loc('ms_label')] = 'H'\n",
        "        else:\n",
        "            if highs[i] > highs[last_hi]:\n",
        "                d.iloc[i, d.columns.get_loc('ms_label')] = 'HH'\n",
        "            else:\n",
        "                d.iloc[i, d.columns.get_loc('ms_label')] = 'LH'\n",
        "        last_hi = i\n",
        "    \n",
        "    # Label lows\n",
        "    for i in piv_lo_idx:\n",
        "        if last_lo is None:\n",
        "            d.iloc[i, d.columns.get_loc('ms_label')] = 'L'\n",
        "        else:\n",
        "            if lows[i] > lows[last_lo]:\n",
        "                d.iloc[i, d.columns.get_loc('ms_label')] = 'HL'\n",
        "            else:\n",
        "                d.iloc[i, d.columns.get_loc('ms_label')] = 'LL'\n",
        "        last_lo = i\n",
        "    \n",
        "    return d\n",
        "\n",
        "def check_ichimoku_exit(row):\n",
        "    \"\"\"\n",
        "    Exit conditions:\n",
        "    - Price falls below the cloud OR\n",
        "    - RSI becomes overbought (> 70)\n",
        "    \"\"\"\n",
        "    below_cloud = (row['close'] < row['senkou_span_a']) and (row['close'] < row['senkou_span_b'])\n",
        "    rsi_overbought = row['RSI'] > 70\n",
        "    \n",
        "    return below_cloud or rsi_overbought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Updated Ichimoku functions loaded with trend filter and Kijun break\n"
          ]
        }
      ],
      "source": [
        "# âœ… GÃœNCELLENM Ä°Åž FONKSÄ°YONLAR: Trend Filter + Kijun Break + MS Lookback Adjustment\n",
        "\n",
        "def trend_filter(row):\n",
        "    \"\"\"\n",
        "    Trend filter: Price above EMA200 and ADX > 20 (strong trend)\n",
        "    \"\"\"\n",
        "    return (row['close'] > row['EMA200']) and (row['ADX'] > 20)\n",
        "\n",
        "def check_ichimoku_entry_updated(row):\n",
        "    \"\"\"\n",
        "    Entry conditions with trend filter:\n",
        "    - Tenkan-sen > Kijun-sen (bullish TK cross)\n",
        "    - Price is above the cloud (Close > both Span A and Span B)\n",
        "    - Chikou Span > Price (Current close > close 26 periods ago)\n",
        "    - Market structure is Uptrend (trend=1)\n",
        "    - Trend filter: Price > EMA200 and ADX > 20\n",
        "    \"\"\"\n",
        "    tk_bullish = row['tenkan_sen'] > row['kijun_sen']\n",
        "    above_cloud = (row['close'] > row['senkou_span_a']) and (row['close'] > row['senkou_span_b'])\n",
        "    uptrend = row['trend'] == 1\n",
        "    \n",
        "    # Chikou filter\n",
        "    chikou_ok = pd.notna(row.get('close_26_back')) and (row['close'] > row['close_26_back'])\n",
        "    \n",
        "    # Trend filter\n",
        "    trend_ok = trend_filter(row)\n",
        "    \n",
        "    return tk_bullish and above_cloud and uptrend and chikou_ok and trend_ok\n",
        "\n",
        "def check_ichimoku_exit_updated(row):\n",
        "    \"\"\"\n",
        "    Exit conditions with Kijun break:\n",
        "    - Price falls below the cloud OR\n",
        "    - Price breaks below Kijun-sen (chop exit) OR\n",
        "    - RSI becomes overbought (> 70)\n",
        "    \"\"\"\n",
        "    below_cloud = (row['close'] < row['senkou_span_a']) and (row['close'] < row['senkou_span_b'])\n",
        "    kijun_break = row['close'] < row['kijun_sen']\n",
        "    rsi_overbought = row['RSI'] > 70\n",
        "    \n",
        "    return below_cloud or kijun_break or rsi_overbought\n",
        "\n",
        "print(\"âœ… Updated Ichimoku functions loaded with trend filter and Kijun break\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Updated run_test_case_2 function loaded\n"
          ]
        }
      ],
      "source": [
        "def run_test_case_2_updated():\n",
        "    \"\"\"\n",
        "    Test Case 2: Ichimoku + Market Structure Strategy (UPDATED)\n",
        "    - Entry: TK bullish cross + Price above cloud + Uptrend + EMA200/ADX filter\n",
        "    - Exit: Price below cloud OR Kijun break OR RSI > 70\n",
        "    - MS lookback adjusted by timeframe (1h=10, 4h=12, other=20)\n",
        "    \"\"\"\n",
        "    print(\"Running Test Case 2 (UPDATED - Ichimoku + MS + Trend Filter)...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    data_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv') and 'macro' not in f]\n",
        "    \n",
        "    results_tc2 = []\n",
        "    \n",
        "    for file in data_files:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        print(f\"\\nProcessing {symbol_tf}...\")\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        print(f\"  Original data: {len(df)} rows\")\n",
        "        \n",
        "        # Calculate base indicators (RSI, BB, KAMA, SuperTrend, EMA200, ADX)\n",
        "        df = calculate_indicators(df)\n",
        "        \n",
        "        # Calculate Ichimoku components\n",
        "        df = calculate_ichimoku(df)\n",
        "        \n",
        "        # Adjust MS lookback based on timeframe\n",
        "        tf = symbol_tf.split('_')[-1]\n",
        "        lb = 20\n",
        "        if tf == '1h': \n",
        "            lb = 10\n",
        "        elif tf == '4h': \n",
        "            lb = 12\n",
        "        print(f\"  Using MS lookback: {lb} for {tf} timeframe\")\n",
        "        \n",
        "        # Identify market structure with adjusted lookback\n",
        "        df = identify_market_structure(df, lookback=lb)\n",
        "        \n",
        "        # Label market structure (HH/HL/LH/LL)\n",
        "        df = label_market_structure(df)\n",
        "        \n",
        "        # Drop remaining NaN\n",
        "        df.dropna(inplace=True)\n",
        "        \n",
        "        print(f\"  Data after all indicators: {len(df)} rows\")\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"  WARNING: {symbol_tf} has 0 rows!\")\n",
        "            continue\n",
        "        \n",
        "        # Show trend distribution\n",
        "        trend_dist = df['trend'].value_counts()\n",
        "        print(f\"  Trend distribution: Uptrend={trend_dist.get(1, 0)}, Neutral={trend_dist.get(0, 0)}, Downtrend={trend_dist.get(-1, 0)}\")\n",
        "        \n",
        "        # Wrapper functions for updated entry/exit\n",
        "        def entry_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_entry_updated(row)\n",
        "            \n",
        "        def exit_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_exit_updated(row)\n",
        "        \n",
        "        # TC2 doesn't enforce profitable exit rule\n",
        "        pnl, win_rate, trades, _ = run_backtest(df, entry_wrapper, exit_wrapper, enforce_profitable_exit=False)\n",
        "        \n",
        "        results_tc2.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Strategy': 'Ichimoku_MS_TrendFilter',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades\n",
        "        })\n",
        "        \n",
        "    results_df = pd.DataFrame(results_tc2)\n",
        "    results_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Test Case 2 (UPDATED) Complete. Results:\")\n",
        "    print(results_df)\n",
        "    results_df.to_csv('test_case_2_updated_results.csv', index=False)\n",
        "    return results_df\n",
        "\n",
        "print(\"âœ… Updated run_test_case_2 function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Advanced entry/exit functions loaded\n"
          ]
        }
      ],
      "source": [
        "# ðŸš€ GELÄ°ÅžMÄ°Åž ENTRY/EXIT FONKSÄ°YONLARI (TÃœM FÄ°LTRELER DAHÄ°L)\n",
        "\n",
        "def check_ichimoku_entry_advanced(row, timeframe='1h'):\n",
        "    \"\"\"\n",
        "    Advanced entry with ALL filters:\n",
        "    1. Original Ichimoku conditions\n",
        "    2. Trend filter (EMA200 + ADX)\n",
        "    3. Volatility filter (ATR)\n",
        "    4. Min trend duration\n",
        "    5. Volume confirmation\n",
        "    6. Time-based filter\n",
        "    \n",
        "    Args:\n",
        "        row: DataFrame row with timestamp as index\n",
        "        timeframe: '1h', '4h', or '1d'\n",
        "    \"\"\"\n",
        "    # 1. Original Ichimoku + Trend filter (zaten var)\n",
        "    tk_bullish = row['tenkan_sen'] > row['kijun_sen']\n",
        "    above_cloud = (row['close'] > row['senkou_span_a']) and (row['close'] > row['senkou_span_b'])\n",
        "    uptrend = row['trend'] == 1\n",
        "    chikou_ok = pd.notna(row.get('close_26_back')) and (row['close'] > row['close_26_back'])\n",
        "    trend_ok = trend_filter(row)\n",
        "    \n",
        "    base_conditions = tk_bullish and above_cloud and uptrend and chikou_ok and trend_ok\n",
        "    \n",
        "    if not base_conditions:\n",
        "        return False\n",
        "    \n",
        "    # 2. Volatility Filter (sadece 1h ve 4h iÃ§in)\n",
        "    if timeframe in ['1h', '4h']:\n",
        "        # 1h iÃ§in daha dar range, 4h iÃ§in daha geniÅŸ\n",
        "        if timeframe == '1h':\n",
        "            vol_ok = volatility_filter(row, min_atr_pct=0.8, max_atr_pct=4.0)\n",
        "        else:  # 4h\n",
        "            vol_ok = volatility_filter(row, min_atr_pct=0.8, max_atr_pct=5.0)\n",
        "        \n",
        "        if not vol_ok:\n",
        "            return False\n",
        "    \n",
        "    # 3. Min Trend Duration (sadece 1h ve 4h iÃ§in)\n",
        "    if timeframe in ['1h', '4h']:\n",
        "        min_bars = 5 if timeframe == '1h' else 3\n",
        "        duration_ok = min_trend_duration_filter(row, min_bars=min_bars)\n",
        "        \n",
        "        if not duration_ok:\n",
        "            return False\n",
        "    \n",
        "    # 4. Volume Confirmation (sadece 1h ve 4h iÃ§in)\n",
        "    if timeframe in ['1h', '4h']:\n",
        "        # 1h iÃ§in daha yÃ¼ksek volume requirement\n",
        "        min_ratio = 1.3 if timeframe == '1h' else 1.2\n",
        "        volume_ok = volume_confirmation_filter(row, min_volume_ratio=min_ratio)\n",
        "        \n",
        "        if not volume_ok:\n",
        "            return False\n",
        "    \n",
        "    # 5. Time-based Filter (sadece 1h iÃ§in)\n",
        "    if timeframe == '1h':\n",
        "        # row.name timestamp iÃ§erir (index)\n",
        "        timestamp = row.name if hasattr(row, 'name') else None\n",
        "        if timestamp is not None:\n",
        "            time_ok = time_based_filter(timestamp, timeframe=timeframe)\n",
        "            if not time_ok:\n",
        "                return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def check_ichimoku_exit_advanced(row):\n",
        "    \"\"\"\n",
        "    Advanced exit (Kijun break zaten eklendi, aynÄ± kalÄ±yor)\n",
        "    \"\"\"\n",
        "    below_cloud = (row['close'] < row['senkou_span_a']) and (row['close'] < row['senkou_span_b'])\n",
        "    kijun_break = row['close'] < row['kijun_sen']\n",
        "    rsi_overbought = row['RSI'] > 70\n",
        "    \n",
        "    return below_cloud or kijun_break or rsi_overbought\n",
        "\n",
        "print(\"âœ… Advanced entry/exit functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… run_test_case_2_advanced function loaded\n"
          ]
        }
      ],
      "source": [
        "def run_test_case_2_advanced():\n",
        "    \"\"\"\n",
        "    Test Case 2: ADVANCED VERSION\n",
        "    TÃ¼m filtreleri iÃ§erir:\n",
        "    - EMA200 + ADX trend filter\n",
        "    - ATR volatility filter\n",
        "    - Min trend duration\n",
        "    - Volume confirmation\n",
        "    - Time-based filter (1h iÃ§in)\n",
        "    - Kijun break exit\n",
        "    - TF-adjusted MS lookback\n",
        "    \"\"\"\n",
        "    print(\"Running Test Case 2 (ADVANCED - ALL FILTERS)...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    data_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv') and 'macro' not in f]\n",
        "    \n",
        "    results_tc2 = []\n",
        "    \n",
        "    for file in data_files:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Processing {symbol_tf}...\")\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        print(f\"  Original data: {len(df)} rows\")\n",
        "        \n",
        "        # Extract timeframe\n",
        "        tf = symbol_tf.split('_')[-1]\n",
        "        \n",
        "        # Calculate base indicators (includes EMA200, ADX)\n",
        "        df = calculate_indicators(df)\n",
        "        \n",
        "        # Calculate ATR for volatility filter\n",
        "        df = calculate_atr_filter(df, period=14)\n",
        "        \n",
        "        # Calculate Ichimoku\n",
        "        df = calculate_ichimoku(df)\n",
        "        \n",
        "        # Adjust MS lookback based on timeframe\n",
        "        lb = 20\n",
        "        if tf == '1h': \n",
        "            lb = 10\n",
        "        elif tf == '4h': \n",
        "            lb = 12\n",
        "        print(f\"  MS lookback: {lb} for {tf}\")\n",
        "        \n",
        "        # Identify market structure\n",
        "        df = identify_market_structure(df, lookback=lb)\n",
        "        \n",
        "        # Calculate trend duration\n",
        "        df = calculate_trend_duration(df)\n",
        "        \n",
        "        # Calculate volume metrics\n",
        "        df = calculate_volume_metrics(df, ma_period=20)\n",
        "        \n",
        "        # Label market structure\n",
        "        df = label_market_structure(df)\n",
        "        \n",
        "        # Drop NaN\n",
        "        df.dropna(inplace=True)\n",
        "        \n",
        "        print(f\"  Data after all indicators: {len(df)} rows\")\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"  WARNING: {symbol_tf} has 0 rows!\")\n",
        "            continue\n",
        "        \n",
        "        # Show statistics\n",
        "        trend_dist = df['trend'].value_counts()\n",
        "        print(f\"  Trend: Up={trend_dist.get(1, 0)}, Neutral={trend_dist.get(0, 0)}, Down={trend_dist.get(-1, 0)}\")\n",
        "        print(f\"  Avg ATR%: {df['ATR_pct'].mean():.2f}%, Avg Volume Ratio: {df['volume_ratio'].mean():.2f}x\")\n",
        "        print(f\"  Avg Trend Duration: {df['trend_duration'].mean():.1f} bars\")\n",
        "        \n",
        "        # Wrapper functions with timeframe parameter\n",
        "        def entry_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_entry_advanced(row, timeframe=tf)\n",
        "            \n",
        "        def exit_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_exit_advanced(row)\n",
        "        \n",
        "        # Run backtest\n",
        "        pnl, win_rate, trades, trades_list = run_backtest(\n",
        "            df, entry_wrapper, exit_wrapper, \n",
        "            enforce_profitable_exit=False\n",
        "        )\n",
        "        \n",
        "        print(f\"  Results: PnL={pnl:.2f}%, WinRate={win_rate:.2f}%, Trades={trades}\")\n",
        "        \n",
        "        results_tc2.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Strategy': 'Ichimoku_Advanced',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades,\n",
        "            'Avg_ATR%': round(df['ATR_pct'].mean(), 2),\n",
        "            'Avg_Volume_Ratio': round(df['volume_ratio'].mean(), 2)\n",
        "        })\n",
        "        \n",
        "    results_df = pd.DataFrame(results_tc2)\n",
        "    results_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š TEST CASE 2 (ADVANCED) COMPLETE:\")\n",
        "    print(\"=\"*70)\n",
        "    print(results_df)\n",
        "    print(\"\\nðŸ’¾ Saving results to 'test_case_2_advanced_results.csv'...\")\n",
        "    results_df.to_csv('test_case_2_advanced_results.csv', index=False)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "print(\"âœ… run_test_case_2_advanced function loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Master report generator loaded!\n"
          ]
        }
      ],
      "source": [
        "# ðŸš€ MASTER REPORT GENERATOR - RUN ALL REPORTS\n",
        "\n",
        "def generate_complete_project_report():\n",
        "    \"\"\"\n",
        "    Master function to generate all project reporting requirements:\n",
        "    1. TC1 vs TC2 Comparison Table\n",
        "    2. Trade Details for Best Strategies\n",
        "    3. All Visualizations\n",
        "    4. Forward Test Comparison\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\" \"*30 + \"COMPLETE PROJECT REPORT GENERATION\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "    \n",
        "    # 1. Generate TC1 vs TC2 Comparison\n",
        "    print(\"\\nðŸ“Š STEP 1: Generating TC1 vs TC2 Comparison Table...\")\n",
        "    print(\"-\"*100)\n",
        "    comparison_df = generate_tc1_vs_tc2_comparison()\n",
        "    \n",
        "    # 2. Generate Trade Details for Top 3 Strategies\n",
        "    print(\"\\n\\nðŸ“‹ STEP 2: Generating Trade Details for Best Strategies...\")\n",
        "    print(\"-\"*100)\n",
        "    \n",
        "    # Best TC1 strategies\n",
        "    strategies_to_detail = [\n",
        "        ('ETHUSDT_1d', check_rsi_entry, check_bb_exit_B, {'threshold': 40.0}, {'near_ratio': 0.85}, 'TC1_RSI_BB'),\n",
        "        ('BTCUSDT_1d', check_rsi_entry, check_bb_exit_B, {'threshold': 40.0}, {'near_ratio': 0.85}, 'TC1_RSI_BB'),\n",
        "        ('ETHUSDT_4h', check_rsi_entry, check_bb_exit_B, {'threshold': 25.0}, {'near_ratio': 0.7}, 'TC1_RSI_BB'),\n",
        "    ]\n",
        "    \n",
        "    for symbol_tf, entry_func, exit_func, entry_params, exit_params, strategy_name in strategies_to_detail:\n",
        "        try:\n",
        "            print(f\"\\n  ðŸ“ˆ Generating trade details for {symbol_tf} - {strategy_name}...\")\n",
        "            df = pd.read_csv(os.path.join(DATA_DIR, f\"{symbol_tf}.csv\"))\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            df.set_index('timestamp', inplace=True)\n",
        "            df = calculate_indicators(df)\n",
        "            df.dropna(inplace=True)\n",
        "            \n",
        "            generate_trade_details_table(df, entry_func, exit_func, \n",
        "                                          entry_params, exit_params,\n",
        "                                          strategy_name, symbol_tf)\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ Error generating trade details for {symbol_tf}: {e}\")\n",
        "    \n",
        "    # 3. Generate All Visualizations\n",
        "    print(\"\\n\\nðŸŽ¨ STEP 3: Generating All Visualizations...\")\n",
        "    print(\"-\"*100)\n",
        "    generate_all_visualizations()\n",
        "    \n",
        "    # 4. Generate Forward Test Comparison\n",
        "    print(\"\\n\\nðŸ”® STEP 4: Generating Forward Test Comparison...\")\n",
        "    print(\"-\"*100)\n",
        "    forward_df = generate_forward_test_comparison()\n",
        "    \n",
        "    # 5. Summary\n",
        "    print(\"\\n\\n\" + \"=\"*100)\n",
        "    print(\" \"*35 + \"REPORT GENERATION COMPLETE!\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"\\nðŸ“ Generated Files:\")\n",
        "    print(\"   CSV Files:\")\n",
        "    print(\"     - tc1_vs_tc2_comparison.csv (Comprehensive comparison table)\")\n",
        "    print(\"     - forward_test_tc1_vs_tc2.csv (Forward test Oct-Nov 2025)\")\n",
        "    print(\"     - *_trade_details.csv (Detailed trade-by-trade records)\")\n",
        "    print(\"\\n   PNG Files:\")\n",
        "    print(\"     - 6x TC2_Ichimoku_signals.png (All timeframes with HH/HL/LH/LL)\")\n",
        "    print(\"     - performance_comparison.png (TC1 vs TC2 bar chart)\")\n",
        "    print(\"\\nâœ… All reporting requirements satisfied!\")\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "print(\"âœ… Master report generator loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Test Case 2 (Ichimoku + Market Structure)...\n",
            "======================================================================\n",
            "\n",
            "Processing BTCUSDT_1d...\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 229 rows\n",
            "  Trend distribution: Uptrend=107, Neutral=58, Downtrend=64\n",
            "\n",
            "Processing BTCUSDT_1h...\n",
            "  Original data: 34348 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 6803 rows\n",
            "  Trend distribution: Uptrend=2395, Neutral=2170, Downtrend=2238\n",
            "\n",
            "Processing BTCUSDT_4h...\n",
            "  Original data: 8588 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 1694 rows\n",
            "  Trend distribution: Uptrend=617, Neutral=520, Downtrend=557\n",
            "\n",
            "Processing ETHUSDT_1d...\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 239 rows\n",
            "  Trend distribution: Uptrend=95, Neutral=58, Downtrend=86\n",
            "\n",
            "Processing ETHUSDT_1h...\n",
            "  Original data: 34348 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 6770 rows\n",
            "  Trend distribution: Uptrend=2500, Neutral=2017, Downtrend=2253\n",
            "\n",
            "Processing ETHUSDT_4h...\n",
            "  Original data: 8588 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  Data after all indicators: 1647 rows\n",
            "  Trend distribution: Uptrend=596, Neutral=496, Downtrend=555\n",
            "Test Case 2 Complete. Results:\n",
            "    Symbol_TF                  Strategy   PnL%  Win_Rate%  Trades\n",
            "0  BTCUSDT_1d  Ichimoku_MarketStructure  -8.10      56.67      30\n",
            "2  BTCUSDT_4h  Ichimoku_MarketStructure -43.23      48.21     168\n",
            "3  ETHUSDT_1d  Ichimoku_MarketStructure -53.61      40.00      25\n",
            "1  BTCUSDT_1h  Ichimoku_MarketStructure -84.51      42.68     574\n",
            "5  ETHUSDT_4h  Ichimoku_MarketStructure -88.74      42.76     145\n",
            "4  ETHUSDT_1h  Ichimoku_MarketStructure -95.67      44.09     626\n"
          ]
        }
      ],
      "source": [
        "def run_test_case_2():\n",
        "    \"\"\"\n",
        "    Test Case 2: Ichimoku + Market Structure Strategy\n",
        "    Entry: TK bullish cross + Price above cloud + Uptrend\n",
        "    Exit: Price below cloud OR RSI > 70\n",
        "    \"\"\"\n",
        "    print(\"Running Test Case 2 (Ichimoku + Market Structure)...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    data_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv') and 'macro' not in f]\n",
        "    \n",
        "    results_tc2 = []\n",
        "    \n",
        "    for file in data_files:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        print(f\"\\nProcessing {symbol_tf}...\")\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        print(f\"  Original data: {len(df)} rows\")\n",
        "        \n",
        "        # Calculate base indicators (RSI, BB, KAMA, SuperTrend)\n",
        "        # Note: calculate_indicators already drops NaN internally\n",
        "        df = calculate_indicators(df)\n",
        "        \n",
        "        # Calculate Ichimoku components\n",
        "        df = calculate_ichimoku(df)\n",
        "        \n",
        "        # Identify market structure (trend)\n",
        "        df = identify_market_structure(df)\n",
        "        \n",
        "        # Label market structure (HH/HL/LH/LL)\n",
        "        df = label_market_structure(df)\n",
        "        \n",
        "        # Drop remaining NaN (from Ichimoku warmup ~52 periods)\n",
        "        df.dropna(inplace=True)\n",
        "        \n",
        "        print(f\"  Data after all indicators: {len(df)} rows\")\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"  WARNING: {symbol_tf} has 0 rows!\")\n",
        "            continue\n",
        "        \n",
        "        # Show trend distribution\n",
        "        trend_dist = df['trend'].value_counts()\n",
        "        print(f\"  Trend distribution: Uptrend={trend_dist.get(1, 0)}, Neutral={trend_dist.get(0, 0)}, Downtrend={trend_dist.get(-1, 0)}\")\n",
        "        \n",
        "        # Wrapper functions to match backtest signature\n",
        "        def entry_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_entry(row)\n",
        "            \n",
        "        def exit_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_exit(row)\n",
        "        \n",
        "        # TC2 doesn't enforce profitable exit rule (different from TC1)\n",
        "        pnl, win_rate, trades, _ = run_backtest(df, entry_wrapper, exit_wrapper, enforce_profitable_exit=False)\n",
        "        \n",
        "        results_tc2.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Strategy': 'Ichimoku_MarketStructure',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades\n",
        "        })\n",
        "        \n",
        "    results_df = pd.DataFrame(results_tc2)\n",
        "    results_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    print(\"Test Case 2 Complete. Results:\")\n",
        "    print(results_df)\n",
        "    results_df.to_csv('test_case_2_results.csv', index=False)\n",
        "    return results_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tc2_results = run_test_case_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Case 3: Forward Test (Oct-Nov 2025)\n",
        "Selecting the best model from TC1/TC2 and testing on the most recent data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Test Case 3 (Forward Test - Oct-Nov 2025)...\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š Best models from each test case:\n",
            "\n",
            "TC1 (Grid Search) - Top 3:\n",
            "    Symbol_TF Entry  Entry_Param       Exit  Exit_Param    PnL%  Win_Rate%  Trades\n",
            "0  ETHUSDT_1d   RSI         40.0  BB_Exit_B        0.85  431.04      94.44      18\n",
            "1  BTCUSDT_1d   RSI         40.0  BB_Exit_B        0.85  243.27      94.74      19\n",
            "2  BTCUSDT_1d   RSI         40.0  BB_Exit_B        0.80  237.67      94.74      19\n",
            "\n",
            "TC2 (Ichimoku+MS) - Top 3:\n",
            "    Symbol_TF                  Strategy   PnL%  Win_Rate%  Trades\n",
            "0  BTCUSDT_1d  Ichimoku_MarketStructure  -8.10      56.67      30\n",
            "1  BTCUSDT_4h  Ichimoku_MarketStructure -43.23      48.21     168\n",
            "2  ETHUSDT_1d  Ichimoku_MarketStructure -53.61      40.00      25\n",
            "\n",
            "ðŸ”® Forward Test Period: 2025-10-01 to 2025-11-26\n",
            "======================================================================\n",
            "\n",
            "ðŸ“ˆ Testing TC1 Strategies on Forward Period:\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Cannot set a DataFrame with multiple columns to the single column EMA200",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8016\\2695031213.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    147\u001b[39m \n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_df\n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m\"__main__\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     forward_results = run_forward_test()\n",
            "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8016\\2695031213.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m             print(f\"  {symbol_tf}: Insufficient data for forward period\")\n\u001b[32m     51\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m# Calculate indicators\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         df_forward = calculate_indicators(df_forward)\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(df_forward) == \u001b[32m0\u001b[39m:\n\u001b[32m     57\u001b[39m             print(f\"  {symbol_tf}: No data after indicator calculation\")\n",
            "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8016\\3452194561.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     32\u001b[39m         direction_col = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m st.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'SUPERTd'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m col][\u001b[32m0\u001b[39m]\n\u001b[32m     33\u001b[39m         df[\u001b[33m'SUPERTd_10_3.0'\u001b[39m] = st[direction_col]\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# EMA200 for trend filter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     df[\u001b[33m'EMA200'\u001b[39m] = df.ta.ema(length=\u001b[32m200\u001b[39m)\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# ADX for trend strength\u001b[39;00m\n\u001b[32m     39\u001b[39m     adx = df.ta.adx(length=\u001b[32m14\u001b[39m)\n",
            "\u001b[32md:\\economics\\env\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4308\u001b[39m             self._setitem_frame(key, value)\n\u001b[32m   4309\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(key, (Series, np.ndarray, list, Index)):\n\u001b[32m   4310\u001b[39m             self._setitem_array(key, value)\n\u001b[32m   4311\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(value, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m4312\u001b[39m             self._set_item_frame_value(key, value)\n\u001b[32m   4313\u001b[39m         elif (\n\u001b[32m   4314\u001b[39m             is_list_like(value)\n\u001b[32m   4315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.columns.is_unique\n",
            "\u001b[32md:\\economics\\env\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4466\u001b[39m \n\u001b[32m   4467\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.isetitem(locs, value)\n\u001b[32m   4468\u001b[39m \n\u001b[32m   4469\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(value.columns) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4470\u001b[39m             raise ValueError(\n\u001b[32m   4471\u001b[39m                 \u001b[33m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[39m\n\u001b[32m   4472\u001b[39m                 f\"column {key}\"\n\u001b[32m   4473\u001b[39m             )\n",
            "\u001b[31mValueError\u001b[39m: Cannot set a DataFrame with multiple columns to the single column EMA200"
          ]
        }
      ],
      "source": [
        "def run_forward_test():\n",
        "    \"\"\"\n",
        "    Test Case 3: Forward Test\n",
        "    - Selects best performing strategy from TC1 and TC2\n",
        "    - Filters data for Oct-Nov 2025\n",
        "    - Compares forward test performance to historical backtest\n",
        "    \"\"\"\n",
        "    print(\"Running Test Case 3 (Forward Test - Oct-Nov 2025)...\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load best results from TC1 and TC2\n",
        "    tc1_results = pd.read_csv('grid_search_results.csv')\n",
        "    tc2_results = pd.read_csv('test_case_2_results.csv')\n",
        "    \n",
        "    print(\"\\nðŸ“Š Best models from each test case:\")\n",
        "    print(\"\\nTC1 (Grid Search) - Top 3:\")\n",
        "    print(tc1_results.head(3).to_string())\n",
        "    print(\"\\nTC2 (Ichimoku+MS) - Top 3:\")\n",
        "    print(tc2_results.head(3).to_string())\n",
        "    \n",
        "    # Forward test period: Oct 1, 2025 - Nov 26, 2025\n",
        "    forward_start = pd.Timestamp('2025-10-01')\n",
        "    forward_end = pd.Timestamp('2025-11-26')\n",
        "    \n",
        "    print(f\"\\nðŸ”® Forward Test Period: {forward_start.date()} to {forward_end.date()}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    forward_results = []\n",
        "    \n",
        "    # Test best TC1 strategies on forward period\n",
        "    print(\"\\nðŸ“ˆ Testing TC1 Strategies on Forward Period:\")\n",
        "    \n",
        "    # Best TC1: RSI Entry + RSI Exit on different timeframes\n",
        "    best_tc1_configs = [\n",
        "        ('BTCUSDT_1d', 'RSI', 30, 'RSI', 70),\n",
        "        ('ETHUSDT_1d', 'RSI', 25, 'RSI', 50),\n",
        "        ('BTCUSDT_1d', 'BB_Reversal', None, 'SuperTrend', None),\n",
        "    ]\n",
        "    \n",
        "    for symbol_tf, entry_name, entry_param, exit_name, exit_param in best_tc1_configs:\n",
        "        file = f\"{symbol_tf}.csv\"\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        # Filter to forward period\n",
        "        df_forward = df[(df.index >= forward_start) & (df.index <= forward_end)].copy()\n",
        "        \n",
        "        if len(df_forward) < 10:\n",
        "            print(f\"  {symbol_tf}: Insufficient data for forward period\")\n",
        "            continue\n",
        "        \n",
        "        # Calculate indicators\n",
        "        df_forward = calculate_indicators(df_forward)\n",
        "        \n",
        "        if len(df_forward) == 0:\n",
        "            print(f\"  {symbol_tf}: No data after indicator calculation\")\n",
        "            continue\n",
        "        \n",
        "        # Select entry/exit functions\n",
        "        entry_funcs = {'RSI': check_rsi_entry, 'BB_Reversal': check_bb_entry, 'KAMA_SuperTrend': check_kama_supertrend}\n",
        "        exit_funcs = {'RSI': check_rsi_exit, 'SuperTrend': check_supertrend_exit, 'BB_Exit_A': check_bb_exit_A, 'BB_Exit_B': check_bb_exit_B}\n",
        "        \n",
        "        entry_func = entry_funcs[entry_name]\n",
        "        exit_func = exit_funcs[exit_name]\n",
        "        \n",
        "        e_p = {'threshold': entry_param} if entry_name == 'RSI' else {}\n",
        "        \n",
        "        if exit_name == 'RSI':\n",
        "            x_p = {'threshold': exit_param}\n",
        "        elif exit_name == 'BB_Exit_B':\n",
        "            x_p = {'near_ratio': exit_param} if exit_param is not None else {}\n",
        "        else:\n",
        "            x_p = {}\n",
        "        \n",
        "        # TC1 uses profitable exit enforcement\n",
        "        pnl, win_rate, trades, _ = run_backtest(df_forward, entry_func, exit_func, e_p, x_p, enforce_profitable_exit=True)\n",
        "        \n",
        "        strategy_name = f\"{entry_name}({entry_param})+{exit_name}({exit_param})\"\n",
        "        print(f\"  {symbol_tf} - {strategy_name}: PnL={pnl:.2f}%, WinRate={win_rate:.2f}%, Trades={trades}\")\n",
        "        \n",
        "        forward_results.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Test_Case': 'TC1',\n",
        "            'Strategy': strategy_name,\n",
        "            'Period': 'Forward (Oct-Nov 2025)',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades\n",
        "        })\n",
        "    \n",
        "    # Test TC2 (Ichimoku+MS) on forward period\n",
        "    print(\"\\nðŸ“ˆ Testing TC2 (Ichimoku+MS) on Forward Period:\")\n",
        "    \n",
        "    for file in ['BTCUSDT_4h.csv', 'ETHUSDT_1h.csv', 'BTCUSDT_1d.csv']:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        # Filter to forward period\n",
        "        df_forward = df[(df.index >= forward_start) & (df.index <= forward_end)].copy()\n",
        "        \n",
        "        if len(df_forward) < 10:\n",
        "            print(f\"  {symbol_tf}: Insufficient data for forward period\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"  {symbol_tf}: Forward data points = {len(df_forward)}\")\n",
        "        \n",
        "        # Calculate all indicators\n",
        "        df_forward = calculate_indicators(df_forward)\n",
        "        df_forward = calculate_ichimoku(df_forward)\n",
        "        df_forward = identify_market_structure(df_forward)\n",
        "        df_forward = label_market_structure(df_forward)\n",
        "        df_forward.dropna(inplace=True)\n",
        "        \n",
        "        if len(df_forward) == 0:\n",
        "            print(f\"  {symbol_tf}: No data after all indicators\")\n",
        "            continue\n",
        "        \n",
        "        # TC2 doesn't enforce profitable exit\n",
        "        pnl, win_rate, trades, _ = run_backtest(df_forward, check_ichimoku_entry, check_ichimoku_exit, enforce_profitable_exit=False)\n",
        "        \n",
        "        print(f\"  {symbol_tf} - Ichimoku+MS: PnL={pnl:.2f}%, WinRate={win_rate:.2f}%, Trades={trades}\")\n",
        "        \n",
        "        forward_results.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Test_Case': 'TC2',\n",
        "            'Strategy': 'Ichimoku+MarketStructure',\n",
        "            'Period': 'Forward (Oct-Nov 2025)',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades\n",
        "        })\n",
        "    \n",
        "    # Summary\n",
        "    forward_df = pd.DataFrame(forward_results)\n",
        "    forward_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š FORWARD TEST RESULTS SUMMARY (Oct-Nov 2025):\")\n",
        "    print(\"=\"*70)\n",
        "    print(forward_df.to_string(index=False))\n",
        "    \n",
        "    forward_df.to_csv('forward_test_results.csv', index=False)\n",
        "    print(\"\\nResults saved to 'forward_test_results.csv'\")\n",
        "    \n",
        "    return forward_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    forward_results = run_forward_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… run_test_case_2_with_MTF function loaded (now with ADX filter)!\n"
          ]
        }
      ],
      "source": [
        "def run_test_case_2_with_MTF():\n",
        "    \"\"\"\n",
        "    ðŸ”¥ Test Case 2 with MULTI-TIMEFRAME + ADX FILTER\n",
        "    \n",
        "    1d timeframe'den trend bilgisini alÄ±r ve 4h/1h stratejilerine uygular.\n",
        "    Bu, alt timeframe'lerin ana trendin tersine ve yatay piyasada iÅŸlem yapmasÄ±nÄ± engeller.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”¥ Running Test Case 2 with MULTI-TIMEFRAME + ADX FILTER...\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Filters applied:\")\n",
        "    print(\"  âœ… EMA200 + ADX trend filter (local)\")\n",
        "    print(\"  âœ… ATR volatility filter\")\n",
        "    print(\"  âœ… 1d TREND CONFIRMATION (MTF)\")\n",
        "    print(\"  âœ… AGGRESSIVE ADX FILTER (1h: ADX>30, 4h: ADX>25) - NEW!\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    data_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv') and 'macro' not in f]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for file in data_files:\n",
        "        symbol_tf = file.replace('.csv', '')\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Processing {symbol_tf}...\")\n",
        "        \n",
        "        # Load lower timeframe data (1h or 4h)\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        \n",
        "        # Extract symbol and timeframe\n",
        "        parts = symbol_tf.split('_')\n",
        "        symbol = parts[0]  # 'BTCUSDT' or 'ETHUSDT'\n",
        "        tf = parts[-1]      # '1h', '4h', or '1d'\n",
        "        \n",
        "        print(f\"  Symbol: {symbol}, Timeframe: {tf}\")\n",
        "        print(f\"  Original data: {len(df)} rows\")\n",
        "        \n",
        "        # Calculate base indicators\n",
        "        df = calculate_indicators(df)\n",
        "        df = calculate_atr_filter(df, period=14)\n",
        "        df = calculate_ichimoku(df)\n",
        "        \n",
        "        # Market structure\n",
        "        lb = 20\n",
        "        if tf == '1h': \n",
        "            lb = 10\n",
        "        elif tf == '4h': \n",
        "            lb = 12\n",
        "        df = identify_market_structure(df, lookback=lb)\n",
        "        df = calculate_trend_duration(df)\n",
        "        df = calculate_volume_metrics(df, ma_period=20)\n",
        "        df = label_market_structure(df)\n",
        "        \n",
        "        # ðŸ”¥ NEW: Load 1d trend data for lower timeframes\n",
        "        if tf in ['1h', '4h']:\n",
        "            print(f\"  ðŸ“Š Loading 1d trend data for {symbol}...\")\n",
        "            try:\n",
        "                df_1d_trend = load_1d_trend_data(symbol)\n",
        "                \n",
        "                # Merge 1d trend into current timeframe\n",
        "                df = df.reset_index()\n",
        "                df = pd.merge_asof(\n",
        "                    df.sort_values('timestamp'), \n",
        "                    df_1d_trend.sort_values('timestamp'), \n",
        "                    on='timestamp', \n",
        "                    direction='backward'\n",
        "                )\n",
        "                df.set_index('timestamp', inplace=True)\n",
        "                \n",
        "                # Show trend distribution from 1d\n",
        "                trend_1d_dist = df['trend_1d'].value_counts()\n",
        "                print(f\"  1d Trend Distribution: Up={trend_1d_dist.get(1, 0)}, Neutral={trend_1d_dist.get(0, 0)}, Down={trend_1d_dist.get(-1, 0)}\")\n",
        "                \n",
        "            except FileNotFoundError:\n",
        "                print(f\"  âš ï¸ WARNING: {symbol}_1d.csv not found! Skipping MTF filter for this pair.\")\n",
        "                df['trend_1d'] = 1  # Default to allow all trades (fallback)\n",
        "        else:\n",
        "            # For 1d timeframe, no MTF filter needed (it IS the higher timeframe)\n",
        "            print(f\"  â„¹ï¸ 1d timeframe - MTF filter not applicable\")\n",
        "        \n",
        "        # Drop NaN\n",
        "        df.dropna(inplace=True)\n",
        "        print(f\"  Data after all indicators & MTF merge: {len(df)} rows\")\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            print(f\"  âš ï¸ WARNING: {symbol_tf} has 0 rows after processing!\")\n",
        "            continue\n",
        "        \n",
        "        # Show statistics\n",
        "        trend_dist = df['trend'].value_counts()\n",
        "        print(f\"  Local Trend: Up={trend_dist.get(1, 0)}, Neutral={trend_dist.get(0, 0)}, Down={trend_dist.get(-1, 0)}\")\n",
        "        print(f\"  Avg ATR%: {df['ATR_pct'].mean():.2f}%, Avg Volume Ratio: {df['volume_ratio'].mean():.2f}x\")\n",
        "        \n",
        "        # Wrapper functions with MTF filter\n",
        "        def entry_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_entry_MTF(row, timeframe=tf)\n",
        "            \n",
        "        def exit_wrapper(row, **kwargs):\n",
        "            return check_ichimoku_exit_advanced(row)\n",
        "        \n",
        "        # Run backtest\n",
        "        pnl, win_rate, trades, trades_list = run_backtest(\n",
        "            df, entry_wrapper, exit_wrapper, \n",
        "            enforce_profitable_exit=False\n",
        "        )\n",
        "        \n",
        "        print(f\"  Results: PnL={pnl:.2f}%, WinRate={win_rate:.2f}%, Trades={trades}\")\n",
        "        \n",
        "        results.append({\n",
        "            'Symbol_TF': symbol_tf,\n",
        "            'Strategy': 'Ichimoku_MTF+ADX',\n",
        "            'PnL%': round(pnl, 2),\n",
        "            'Win_Rate%': round(win_rate, 2),\n",
        "            'Trades': trades,\n",
        "            'Avg_ATR%': round(df['ATR_pct'].mean(), 2) if 'ATR_pct' in df else 0,\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.sort_values(by='PnL%', ascending=False, inplace=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ”¥ TEST CASE 2 (WITH MTF + ADX FILTER) COMPLETE:\")\n",
        "    print(\"=\"*70)\n",
        "    print(results_df)\n",
        "    print(\"\\nðŸ’¾ Saving results to 'test_case_2_MTF_ADX_results.csv'...\")\n",
        "    results_df.to_csv('test_case_2_MTF_ADX_results.csv', index=False)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "print(\"âœ… run_test_case_2_with_MTF function loaded (now with ADX filter)!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Starting MTF + ADX (Multi-Timeframe + Aggressive ADX) filtered backtest...\n",
            "This will add 1d trend confirmation AND aggressive ADX filter to 4h and 1h strategies\n",
            "ADX Thresholds: 1h > 30, 4h > 25 (strong trend required)\n",
            "======================================================================\n",
            "ðŸ”¥ Running Test Case 2 with MULTI-TIMEFRAME + ADX FILTER...\n",
            "======================================================================\n",
            "Filters applied:\n",
            "  âœ… EMA200 + ADX trend filter (local)\n",
            "  âœ… ATR volatility filter\n",
            "  âœ… 1d TREND CONFIRMATION (MTF)\n",
            "  âœ… AGGRESSIVE ADX FILTER (1h: ADX>30, 4h: ADX>25) - NEW!\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing BTCUSDT_1d...\n",
            "  Symbol: BTCUSDT, Timeframe: 1d\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  â„¹ï¸ 1d timeframe - MTF filter not applicable\n",
            "  Data after all indicators & MTF merge: 229 rows\n",
            "  Local Trend: Up=107, Neutral=58, Down=64\n",
            "  Avg ATR%: 3.65%, Avg Volume Ratio: 1.32x\n",
            "  Results: PnL=27.72%, WinRate=51.61%, Trades=31\n",
            "\n",
            "======================================================================\n",
            "Processing BTCUSDT_1h...\n",
            "  Symbol: BTCUSDT, Timeframe: 1h\n",
            "  Original data: 34346 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  ðŸ“Š Loading 1d trend data for BTCUSDT...\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  1d Trend Distribution: Up=11591, Neutral=15696, Down=2283\n",
            "  Data after all indicators & MTF merge: 5896 rows\n",
            "  Local Trend: Up=2045, Neutral=1925, Down=1926\n",
            "  Avg ATR%: 0.68%, Avg Volume Ratio: 1.35x\n",
            "  Results: PnL=-8.09%, WinRate=47.37%, Trades=19\n",
            "\n",
            "======================================================================\n",
            "Processing BTCUSDT_4h...\n",
            "  Symbol: BTCUSDT, Timeframe: 4h\n",
            "  Original data: 8587 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  ðŸ“Š Loading 1d trend data for BTCUSDT...\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  1d Trend Distribution: Up=2898, Neutral=3924, Down=571\n",
            "  Data after all indicators & MTF merge: 1507 rows\n",
            "  Local Trend: Up=566, Neutral=474, Down=467\n",
            "  Avg ATR%: 1.41%, Avg Volume Ratio: 1.36x\n",
            "  Results: PnL=-15.75%, WinRate=37.50%, Trades=40\n",
            "\n",
            "======================================================================\n",
            "Processing ETHUSDT_1d...\n",
            "  Symbol: ETHUSDT, Timeframe: 1d\n",
            "  Original data: 1432 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  â„¹ï¸ 1d timeframe - MTF filter not applicable\n",
            "  Data after all indicators & MTF merge: 239 rows\n",
            "  Local Trend: Up=95, Neutral=58, Down=86\n",
            "  Avg ATR%: 5.06%, Avg Volume Ratio: 1.26x\n",
            "  Results: PnL=14.41%, WinRate=45.00%, Trades=20\n",
            "\n",
            "======================================================================\n",
            "Processing ETHUSDT_1h...\n",
            "  Symbol: ETHUSDT, Timeframe: 1h\n",
            "  Original data: 34346 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  ðŸ“Š Loading 1d trend data for ETHUSDT...\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  1d Trend Distribution: Up=8088, Neutral=12383, Down=9099\n",
            "  Data after all indicators & MTF merge: 5892 rows\n",
            "  Local Trend: Up=2121, Neutral=1819, Down=1952\n",
            "  Avg ATR%: 0.92%, Avg Volume Ratio: 1.41x\n",
            "  Results: PnL=-15.62%, WinRate=52.00%, Trades=25\n",
            "\n",
            "======================================================================\n",
            "Processing ETHUSDT_4h...\n",
            "  Symbol: ETHUSDT, Timeframe: 4h\n",
            "  Original data: 8587 rows\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  ðŸ“Š Loading 1d trend data for ETHUSDT...\n",
            "  Dropped 199 rows due to indicator warmup period\n",
            "  1d Trend Distribution: Up=2022, Neutral=3096, Down=2275\n",
            "  Data after all indicators & MTF merge: 1469 rows\n",
            "  Local Trend: Up=573, Neutral=429, Down=467\n",
            "  Avg ATR%: 1.94%, Avg Volume Ratio: 1.36x\n",
            "  Results: PnL=-32.54%, WinRate=33.33%, Trades=24\n",
            "\n",
            "======================================================================\n",
            "ðŸ”¥ TEST CASE 2 (WITH MTF + ADX FILTER) COMPLETE:\n",
            "======================================================================\n",
            "    Symbol_TF          Strategy   PnL%  Win_Rate%  Trades  Avg_ATR%\n",
            "0  BTCUSDT_1d  Ichimoku_MTF+ADX  27.72      51.61      31      3.65\n",
            "3  ETHUSDT_1d  Ichimoku_MTF+ADX  14.41      45.00      20      5.06\n",
            "1  BTCUSDT_1h  Ichimoku_MTF+ADX  -8.09      47.37      19      0.68\n",
            "4  ETHUSDT_1h  Ichimoku_MTF+ADX -15.62      52.00      25      0.92\n",
            "2  BTCUSDT_4h  Ichimoku_MTF+ADX -15.75      37.50      40      1.41\n",
            "5  ETHUSDT_4h  Ichimoku_MTF+ADX -32.54      33.33      24      1.94\n",
            "\n",
            "ðŸ’¾ Saving results to 'test_case_2_MTF_ADX_results.csv'...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š COMPARISON: Before vs After MTF+ADX Filter\n",
            "======================================================================\n",
            "\n",
            "ðŸ“‰ BEFORE MTF+ADX Filter (Advanced only):\n",
            " Symbol_TF   PnL%  Win_Rate%  Trades\n",
            "BTCUSDT_1d  27.72      51.61      31\n",
            "ETHUSDT_1d  14.41      45.00      20\n",
            "BTCUSDT_1h   1.46      48.10      79\n",
            "BTCUSDT_4h -42.19      34.65     101\n",
            "ETHUSDT_1h -63.37      39.35     155\n",
            "ETHUSDT_4h -63.91      36.84      95\n",
            "\n",
            "ðŸ“ˆ AFTER MTF+ADX Filter (MTF + Aggressive ADX):\n",
            " Symbol_TF   PnL%  Win_Rate%  Trades\n",
            "BTCUSDT_1d  27.72      51.61      31\n",
            "ETHUSDT_1d  14.41      45.00      20\n",
            "BTCUSDT_1h  -8.09      47.37      19\n",
            "ETHUSDT_1h -15.62      52.00      25\n",
            "BTCUSDT_4h -15.75      37.50      40\n",
            "ETHUSDT_4h -32.54      33.33      24\n",
            "\n",
            "ðŸ’¡ IMPROVEMENT:\n",
            "BTCUSDT_1d      | PnL:  27.72% â†’  27.72% ( +0.00%) | WR: 51.61% â†’ 51.61% (+0.00%)\n",
            "ETHUSDT_1d      | PnL:  14.41% â†’  14.41% ( +0.00%) | WR: 45.00% â†’ 45.00% (+0.00%)\n",
            "BTCUSDT_1h      | PnL:   1.46% â†’  -8.09% ( -9.55%) | WR: 48.10% â†’ 47.37% (-0.73%)\n",
            "ETHUSDT_1h      | PnL: -63.37% â†’ -15.62% (+47.75%) | WR: 39.35% â†’ 52.00% (+12.65%)\n",
            "BTCUSDT_4h      | PnL: -42.19% â†’ -15.75% (+26.44%) | WR: 34.65% â†’ 37.50% (+2.85%)\n",
            "ETHUSDT_4h      | PnL: -63.91% â†’ -32.54% (+31.37%) | WR: 36.84% â†’ 33.33% (-3.51%)\n",
            "\n",
            "âœ… MTF+ADX backtest complete!\n"
          ]
        }
      ],
      "source": [
        "# ðŸš€ RUN MTF + ADX FILTERED BACKTEST\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸŽ¯ Starting MTF + ADX (Multi-Timeframe + Aggressive ADX) filtered backtest...\")\n",
        "    print(\"This will add 1d trend confirmation AND aggressive ADX filter to 4h and 1h strategies\")\n",
        "    print(\"ADX Thresholds: 1h > 30, 4h > 25 (strong trend required)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    results_mtf = run_test_case_2_with_MTF()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š COMPARISON: Before vs After MTF+ADX Filter\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load previous results for comparison\n",
        "    try:\n",
        "        df_before = pd.read_csv('test_case_2_advanced_results.csv')\n",
        "        df_after = results_mtf\n",
        "        \n",
        "        print(\"\\nðŸ“‰ BEFORE MTF+ADX Filter (Advanced only):\")\n",
        "        print(df_before[['Symbol_TF', 'PnL%', 'Win_Rate%', 'Trades']].to_string(index=False))\n",
        "        \n",
        "        print(\"\\nðŸ“ˆ AFTER MTF+ADX Filter (MTF + Aggressive ADX):\")\n",
        "        print(df_after[['Symbol_TF', 'PnL%', 'Win_Rate%', 'Trades']].to_string(index=False))\n",
        "        \n",
        "        print(\"\\nðŸ’¡ IMPROVEMENT:\")\n",
        "        for _, row_after in df_after.iterrows():\n",
        "            symbol_tf = row_after['Symbol_TF']\n",
        "            row_before = df_before[df_before['Symbol_TF'] == symbol_tf]\n",
        "            \n",
        "            if not row_before.empty:\n",
        "                pnl_before = row_before['PnL%'].values[0]\n",
        "                pnl_after = row_after['PnL%']\n",
        "                wr_before = row_before['Win_Rate%'].values[0]\n",
        "                wr_after = row_after['Win_Rate%']\n",
        "                \n",
        "                pnl_change = pnl_after - pnl_before\n",
        "                wr_change = wr_after - wr_before\n",
        "                \n",
        "                print(f\"{symbol_tf:15} | PnL: {pnl_before:6.2f}% â†’ {pnl_after:6.2f}% ({pnl_change:+6.2f}%) | WR: {wr_before:5.2f}% â†’ {wr_after:5.2f}% ({wr_change:+5.2f}%)\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nâš ï¸ Previous results not found. Only showing MTF+ADX results.\")\n",
        "    \n",
        "    print(\"\\nâœ… MTF+ADX backtest complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Trade details table generator loaded!\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“Š TRADE DETAILS TABLE GENERATOR\n",
        "# Creates detailed trade-by-trade report for any strategy\n",
        "\n",
        "def generate_trade_details_table(df, entry_func, exit_func, entry_params={}, exit_params={}, \n",
        "                                  strategy_name=\"Strategy\", symbol_tf=\"\", enforce_profitable_exit=False):\n",
        "    \"\"\"\n",
        "    Generate detailed trade-by-trade table with entry/exit prices, dates, PnL, etc.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with columns: Trade_ID, Entry_Date, Entry_Price, Exit_Date, Exit_Price, \n",
        "                                Duration_Days, PnL%, Cumulative_PnL%, Trade_Result\n",
        "    \"\"\"\n",
        "    trades_list = []\n",
        "    in_position = False\n",
        "    entry_price = 0\n",
        "    entry_date = None\n",
        "    trade_id = 0\n",
        "    cumulative_pnl = 0\n",
        "    \n",
        "    for i in range(1, len(df)):\n",
        "        row = df.iloc[i]\n",
        "        prev_row = df.iloc[i-1]\n",
        "        \n",
        "        if not in_position:\n",
        "            # Check entry\n",
        "            entry_signal = entry_func(row, **entry_params)\n",
        "            if entry_signal:\n",
        "                in_position = True\n",
        "                entry_price = row['close']\n",
        "                entry_date = row.name if hasattr(row, 'name') else df.index[i]\n",
        "                trade_id += 1\n",
        "        else:\n",
        "            # Check exit\n",
        "            exit_signal = exit_func(row, **exit_params)\n",
        "            \n",
        "            # Enforce profitable exit if needed\n",
        "            if enforce_profitable_exit and not exit_signal:\n",
        "                if row['close'] > entry_price:\n",
        "                    exit_signal = True\n",
        "            \n",
        "            if exit_signal or i == len(df) - 1:  # Exit on signal or end of data\n",
        "                exit_price = row['close']\n",
        "                exit_date = row.name if hasattr(row, 'name') else df.index[i]\n",
        "                \n",
        "                # Calculate metrics\n",
        "                pnl_pct = ((exit_price - entry_price) / entry_price) * 100\n",
        "                cumulative_pnl += pnl_pct\n",
        "                duration = (exit_date - entry_date).days if hasattr(exit_date, 'days') else 0\n",
        "                \n",
        "                trade_result = \"WIN\" if pnl_pct > 0 else \"LOSS\"\n",
        "                \n",
        "                trades_list.append({\n",
        "                    'Trade_ID': trade_id,\n",
        "                    'Entry_Date': entry_date,\n",
        "                    'Entry_Price': round(entry_price, 2),\n",
        "                    'Exit_Date': exit_date,\n",
        "                    'Exit_Price': round(exit_price, 2),\n",
        "                    'Duration_Days': duration,\n",
        "                    'PnL%': round(pnl_pct, 2),\n",
        "                    'Cumulative_PnL%': round(cumulative_pnl, 2),\n",
        "                    'Result': trade_result\n",
        "                })\n",
        "                \n",
        "                in_position = False\n",
        "    \n",
        "    trades_df = pd.DataFrame(trades_list)\n",
        "    \n",
        "    if len(trades_df) > 0:\n",
        "        # Save to CSV\n",
        "        filename = f\"{symbol_tf}_{strategy_name}_trade_details.csv\"\n",
        "        trades_df.to_csv(filename, index=False)\n",
        "        print(f\"âœ… Trade details saved to {filename}\")\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"TRADE DETAILS SUMMARY: {symbol_tf} - {strategy_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total Trades: {len(trades_df)}\")\n",
        "        print(f\"Winning Trades: {(trades_df['Result'] == 'WIN').sum()} ({(trades_df['Result'] == 'WIN').sum() / len(trades_df) * 100:.2f}%)\")\n",
        "        print(f\"Losing Trades: {(trades_df['Result'] == 'LOSS').sum()}\")\n",
        "        print(f\"Average PnL per Trade: {trades_df['PnL%'].mean():.2f}%\")\n",
        "        print(f\"Best Trade: {trades_df['PnL%'].max():.2f}%\")\n",
        "        print(f\"Worst Trade: {trades_df['PnL%'].min():.2f}%\")\n",
        "        print(f\"Average Duration: {trades_df['Duration_Days'].mean():.1f} days\")\n",
        "        print(f\"Final Cumulative PnL: {trades_df['Cumulative_PnL%'].iloc[-1]:.2f}%\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Display first 10 and last 10 trades\n",
        "        print(\"First 10 Trades:\")\n",
        "        print(trades_df.head(10).to_string(index=False))\n",
        "        if len(trades_df) > 10:\n",
        "            print(f\"\\n... ({len(trades_df) - 20} trades omitted) ...\\n\")\n",
        "            print(\"Last 10 Trades:\")\n",
        "            print(trades_df.tail(10).to_string(index=False))\n",
        "    \n",
        "    return trades_df\n",
        "\n",
        "print(\"âœ… Trade details table generator loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def plot_trading_signals(df, symbol_tf, entry_func, exit_func, entry_params={}, exit_params={}, show_ichimoku=False, show_ms_labels=False):\n",
        "    \"\"\"\n",
        "    Plots price chart with buy/sell signals and optionally Ichimoku Cloud and Market Structure labels.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(16, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
        "    \n",
        "    ax1 = axes[0]\n",
        "    ax2 = axes[1]\n",
        "    \n",
        "    # Plot price\n",
        "    ax1.plot(df.index, df['close'], label='Close Price', color='black', linewidth=1)\n",
        "    \n",
        "    # Plot Ichimoku Cloud if requested\n",
        "    if show_ichimoku and 'senkou_span_a' in df.columns:\n",
        "        ax1.fill_between(df.index, df['senkou_span_a'], df['senkou_span_b'], \n",
        "                         where=df['senkou_span_a'] >= df['senkou_span_b'],\n",
        "                         color='green', alpha=0.2, label='Bullish Cloud')\n",
        "        ax1.fill_between(df.index, df['senkou_span_a'], df['senkou_span_b'], \n",
        "                         where=df['senkou_span_a'] < df['senkou_span_b'],\n",
        "                         color='red', alpha=0.2, label='Bearish Cloud')\n",
        "        ax1.plot(df.index, df['tenkan_sen'], label='Tenkan-sen', color='blue', linewidth=0.8, alpha=0.7)\n",
        "        ax1.plot(df.index, df['kijun_sen'], label='Kijun-sen', color='red', linewidth=0.8, alpha=0.7)\n",
        "    \n",
        "    # Find buy/sell signals\n",
        "    buy_signals = []\n",
        "    sell_signals = []\n",
        "    position = None\n",
        "    \n",
        "    for i in range(1, len(df)):\n",
        "        row = df.iloc[i]\n",
        "        prev_row = df.iloc[i-1]\n",
        "        \n",
        "        if position:\n",
        "            try:\n",
        "                should_exit = exit_func(row, **exit_params)\n",
        "            except TypeError:\n",
        "                should_exit = exit_func(row)\n",
        "            \n",
        "            if should_exit:\n",
        "                sell_signals.append((df.index[i], row['close']))\n",
        "                position = None\n",
        "        \n",
        "        if not position:\n",
        "            if entry_func.__name__ == 'check_bb_entry':\n",
        "                should_entry = entry_func(row, prev_row)\n",
        "            else:\n",
        "                try:\n",
        "                    should_entry = entry_func(row, **entry_params)\n",
        "                except TypeError:\n",
        "                    should_entry = entry_func(row)\n",
        "            \n",
        "            if should_entry:\n",
        "                buy_signals.append((df.index[i], row['close']))\n",
        "                position = {'price': row['close']}\n",
        "    \n",
        "    # Plot signals\n",
        "    if buy_signals:\n",
        "        buy_x, buy_y = zip(*buy_signals)\n",
        "        ax1.scatter(buy_x, buy_y, marker='^', color='green', s=100, label=f'Buy ({len(buy_signals)})', zorder=5)\n",
        "    \n",
        "    if sell_signals:\n",
        "        sell_x, sell_y = zip(*sell_signals)\n",
        "        ax1.scatter(sell_x, sell_y, marker='v', color='red', s=100, label=f'Sell ({len(sell_signals)})', zorder=5)\n",
        "    \n",
        "    # Plot Market Structure labels if requested\n",
        "    if show_ms_labels and 'ms_label' in df.columns:\n",
        "        for ts, lbl, price in zip(df.index, df['ms_label'], df['close']):\n",
        "            if isinstance(lbl, str) and pd.notna(lbl):\n",
        "                ax1.text(ts, price * 1.002, lbl, fontsize=8, color='darkblue', \n",
        "                        alpha=0.8, ha='center', fontweight='bold')\n",
        "    \n",
        "    ax1.set_title(f'{symbol_tf} - Trading Signals', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Price (USDT)')\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot RSI\n",
        "    ax2.plot(df.index, df['RSI'], label='RSI(14)', color='purple', linewidth=1)\n",
        "    ax2.axhline(y=70, color='red', linestyle='--', alpha=0.5, label='Overbought (70)')\n",
        "    ax2.axhline(y=30, color='green', linestyle='--', alpha=0.5, label='Oversold (30)')\n",
        "    ax2.fill_between(df.index, 30, 70, alpha=0.1, color='gray')\n",
        "    ax2.set_ylabel('RSI')\n",
        "    ax2.set_xlabel('Date')\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.legend(loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Format x-axis\n",
        "    for ax in axes:\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
        "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{symbol_tf}_signals.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Chart saved as '{symbol_tf}_signals.png'\")\n",
        "\n",
        "def plot_performance_comparison():\n",
        "    \"\"\"\n",
        "    Creates a bar chart comparing strategy performance.\n",
        "    \"\"\"\n",
        "    # Load results\n",
        "    tc1 = pd.read_csv('grid_search_results.csv').head(10)\n",
        "    tc2 = pd.read_csv('test_case_2_results.csv')\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # TC1 Top 10\n",
        "    ax1 = axes[0]\n",
        "    labels = [f\"{row['Symbol_TF']}\\n{row['Entry']}({row['Entry_Param']})\" for _, row in tc1.iterrows()]\n",
        "    colors = ['green' if pnl > 0 else 'red' for pnl in tc1['PnL%']]\n",
        "    bars1 = ax1.bar(range(len(tc1)), tc1['PnL%'], color=colors, alpha=0.7)\n",
        "    ax1.set_xticks(range(len(tc1)))\n",
        "    ax1.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
        "    ax1.set_ylabel('PnL (%)')\n",
        "    ax1.set_title('Test Case 1: Grid Search - Top 10 Strategies', fontweight='bold')\n",
        "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars1, tc1['PnL%']):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
        "                 f'{val:.1f}%', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # TC2 Results\n",
        "    ax2 = axes[1]\n",
        "    labels2 = tc2['Symbol_TF'].tolist()\n",
        "    colors2 = ['green' if pnl > 0 else 'red' for pnl in tc2['PnL%']]\n",
        "    bars2 = ax2.bar(range(len(tc2)), tc2['PnL%'], color=colors2, alpha=0.7)\n",
        "    ax2.set_xticks(range(len(tc2)))\n",
        "    ax2.set_xticklabels(labels2, rotation=45, ha='right')\n",
        "    ax2.set_ylabel('PnL (%)')\n",
        "    ax2.set_title('Test Case 2: Ichimoku + Market Structure', fontweight='bold')\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars2, tc2['PnL%']):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
        "                 f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Performance comparison chart saved as 'performance_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… TC1 vs TC2 comparison function loaded!\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“Š TC1 vs TC2 COMPREHENSIVE COMPARISON TABLE\n",
        "\n",
        "def generate_tc1_vs_tc2_comparison():\n",
        "    \"\"\"\n",
        "    Generate comprehensive comparison table between Test Case 1 (Grid Search) \n",
        "    and Test Case 2 (Ichimoku + Market Structure) across all timeframes.\n",
        "    \"\"\"\n",
        "    print(\"=\"*100)\n",
        "    print(\" \"*35 + \"TC1 vs TC2 COMPARISON\")\n",
        "    print(\"=\"*100)\n",
        "    \n",
        "    # Load results\n",
        "    try:\n",
        "        df_tc1 = pd.read_csv('grid_search_results.csv')\n",
        "        df_tc2_original = pd.read_csv('test_case_2_results.csv')\n",
        "        df_tc2_advanced = pd.read_csv('test_case_2_advanced_results.csv')\n",
        "        df_tc2_mtf_adx = pd.read_csv('test_case_2_MTF_ADX_results.csv')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"âš ï¸ Missing file: {e}\")\n",
        "        return None\n",
        "    \n",
        "    # Get best TC1 strategy for each timeframe\n",
        "    timeframes = ['1h', '4h', '1d']\n",
        "    symbols = ['BTCUSDT', 'ETHUSDT']\n",
        "    \n",
        "    comparison_data = []\n",
        "    \n",
        "    for symbol in symbols:\n",
        "        for tf in timeframes:\n",
        "            symbol_tf = f\"{symbol}_{tf}\"\n",
        "            \n",
        "            # Get best TC1 strategy for this symbol_tf\n",
        "            tc1_subset = df_tc1[df_tc1['Symbol_TF'] == symbol_tf].sort_values(by='PnL%', ascending=False)\n",
        "            \n",
        "            if len(tc1_subset) > 0:\n",
        "                best_tc1 = tc1_subset.iloc[0]\n",
        "                tc1_strategy = f\"{best_tc1['Entry']}\"\n",
        "                if 'Entry_Param' in best_tc1 and pd.notna(best_tc1['Entry_Param']):\n",
        "                    tc1_strategy += f\"({best_tc1['Entry_Param']})\"\n",
        "                tc1_strategy += f\" + {best_tc1['Exit']}\"\n",
        "                tc1_pnl = best_tc1['PnL%']\n",
        "                tc1_wr = best_tc1['Win_Rate%']\n",
        "                tc1_trades = best_tc1['Trades']\n",
        "            else:\n",
        "                tc1_strategy = \"N/A\"\n",
        "                tc1_pnl = 0\n",
        "                tc1_wr = 0\n",
        "                tc1_trades = 0\n",
        "            \n",
        "            # Get TC2 results (use MTF+ADX as best version)\n",
        "            tc2_mtf_adx = df_tc2_mtf_adx[df_tc2_mtf_adx['Symbol_TF'] == symbol_tf]\n",
        "            tc2_original = df_tc2_original[df_tc2_original['Symbol_TF'] == symbol_tf]\n",
        "            \n",
        "            if len(tc2_mtf_adx) > 0:\n",
        "                tc2 = tc2_mtf_adx.iloc[0]\n",
        "                tc2_strategy = \"Ichimoku+MS (MTF+ADX)\"\n",
        "                tc2_pnl = tc2['PnL%']\n",
        "                tc2_wr = tc2['Win_Rate%']\n",
        "                tc2_trades = tc2['Trades']\n",
        "            elif len(tc2_original) > 0:\n",
        "                tc2 = tc2_original.iloc[0]\n",
        "                tc2_strategy = \"Ichimoku+MS (Original)\"\n",
        "                tc2_pnl = tc2['PnL%']\n",
        "                tc2_wr = tc2['Win_Rate%']\n",
        "                tc2_trades = tc2['Trades']\n",
        "            else:\n",
        "                tc2_strategy = \"N/A\"\n",
        "                tc2_pnl = 0\n",
        "                tc2_wr = 0\n",
        "                tc2_trades = 0\n",
        "            \n",
        "            # Determine winner\n",
        "            winner = \"TC1\" if tc1_pnl > tc2_pnl else \"TC2\"\n",
        "            pnl_diff = tc1_pnl - tc2_pnl\n",
        "            \n",
        "            comparison_data.append({\n",
        "                'Symbol_TF': symbol_tf,\n",
        "                'TC1_Strategy': tc1_strategy,\n",
        "                'TC1_PnL%': round(tc1_pnl, 2),\n",
        "                'TC1_WinRate%': round(tc1_wr, 2),\n",
        "                'TC1_Trades': tc1_trades,\n",
        "                'TC2_Strategy': tc2_strategy,\n",
        "                'TC2_PnL%': round(tc2_pnl, 2),\n",
        "                'TC2_WinRate%': round(tc2_wr, 2),\n",
        "                'TC2_Trades': tc2_trades,\n",
        "                'Winner': winner,\n",
        "                'PnL_Difference': round(pnl_diff, 2)\n",
        "            })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Save to CSV\n",
        "    comparison_df.to_csv('tc1_vs_tc2_comparison.csv', index=False)\n",
        "    print(\"\\nâœ… Comparison saved to 'tc1_vs_tc2_comparison.csv'\\n\")\n",
        "    \n",
        "    # Print formatted table\n",
        "    print(\"DETAILED COMPARISON TABLE:\")\n",
        "    print(\"=\"*100)\n",
        "    for _, row in comparison_df.iterrows():\n",
        "        print(f\"\\nðŸ“Š {row['Symbol_TF']}:\")\n",
        "        print(f\"   TC1: {row['TC1_Strategy'][:50]}\")\n",
        "        print(f\"        PnL: {row['TC1_PnL%']:>7.2f}% | Win Rate: {row['TC1_WinRate%']:>5.2f}% | Trades: {row['TC1_Trades']:>3}\")\n",
        "        print(f\"   TC2: {row['TC2_Strategy']}\")\n",
        "        print(f\"        PnL: {row['TC2_PnL%']:>7.2f}% | Win Rate: {row['TC2_WinRate%']:>5.2f}% | Trades: {row['TC2_Trades']:>3}\")\n",
        "        print(f\"   ðŸ† WINNER: {row['Winner']} (Difference: {row['PnL_Difference']:+.2f}%)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    \n",
        "    # Summary statistics\n",
        "    tc1_wins = (comparison_df['Winner'] == 'TC1').sum()\n",
        "    tc2_wins = (comparison_df['Winner'] == 'TC2').sum()\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ OVERALL SUMMARY:\")\n",
        "    print(f\"   TC1 Wins: {tc1_wins}/{len(comparison_df)} timeframes\")\n",
        "    print(f\"   TC2 Wins: {tc2_wins}/{len(comparison_df)} timeframes\")\n",
        "    print(f\"   TC1 Avg PnL: {comparison_df['TC1_PnL%'].mean():.2f}%\")\n",
        "    print(f\"   TC2 Avg PnL: {comparison_df['TC2_PnL%'].mean():.2f}%\")\n",
        "    print(f\"   Average Difference: {comparison_df['PnL_Difference'].mean():.2f}%\")\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "print(\"âœ… TC1 vs TC2 comparison function loaded!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
